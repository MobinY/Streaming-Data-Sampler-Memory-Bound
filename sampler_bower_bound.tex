\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{algorithm,algpseudocode,float}
\usepackage{xspace}
%\usepackage{hyperref}
%\usepackage[usenames]{color}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\newcommand{\samp}{\textsf{SAMP}\xspace}
\newcommand{\success}{\textsf{SUCC}\xspace}
\newcommand{\enc}{\textsf{ENC}\xspace}
\newcommand{\dec}{\textsf{DEC}\xspace}
\newcommand{\s}{\textsf{s}\xspace}
\newcommand{\R}{\mathbb{R}}
\newcommand{\sk}{\mathsf{sk}}

\title{An optimal space lower bound for samplers}
\author{Jelani Nelson\thanks{Harvard University. \texttt{minilek@seas.harvard.edu}. Supported by NSF grant IIS-1447471 and
   CAREER award CCF-1350670, ONR Young Investigator award N00014-15-1-2388, and a Google Faculty Research Award.}
  \and Jakub Pachocki\thanks{OpenAI. \texttt{jakub.pachocki@gmail.com}. Work done while affiliated with Harvard University, with the support of ONR grant N00014-15-1-2388.}
  \and Zhengyu Wang\thanks{Harvard University. \texttt{zhengyuwang@g.harvard.edu}. Supported by NSF grant CCF-1350670.}}

\begin{document}

\maketitle

\begin{abstract}
In the $\ell_0$-sampler sketching problem, the goal is to compress a high-dimensional vector $x\in\R^n$ into a low-dimensional {\em sketch} $\sk(x)\in\{0,1\}^S$ so that
\begin{enumerate}
\item $S$, the memory footprint of the sketch, is as small as possible,
\item a third party given only $\sk(x), \sk(y)$, but not $x, y\in\R^n$, is able to form $\sk(x+y)$ for any $x,y\in\R^n$,
\item given access to only $\sk(x)$, a third party can recover an index $i\in[n]$ such that with probability $1-\delta$, $i$ is a uniformly random element in the support of $x$ (i.e.\ $x_i\neq 0$).
\end{enumerate}
We prove that any such sketching mechanism requires $S = \Omega(\log^2 n\cdot \log(1/\delta))$ bits for any $2^{-n^{.99}} < \delta < .1$, which is nearly the full range of $\delta$ of interest and is optimal in this range \cite{JowhariST11}. Our lower bound holds even if the mechanism is allowed to output any element of the support, and not necessarily a uniformly random one. We prove our lower bound by showing that $\ell_0$-samplers which are {\em too} memory-efficient can be used to compress subsets of $[n]$ of certain sizes below the information theoretic minimum. Our compression scheme masks information by injecting noise, and is loosely motivated by ideas in differential privacy. Our correctness analysis involves understanding the ability of such mechanisms to correctly answer adaptive queries which have non-zero, but bounded, mutual information with the randomness of the mechanism, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens.
\end{abstract}


\section{Introduction}
We study the space lower bound for maintaining a sampler over a turnstile stream. An $\ell_p$-sampler with failure probability at most $\delta$ is a randomized data structure for maintaining vector $x\in \mathbb{R}^n$ (initially 0) under a stream of updates in the form of $(i, \Delta)$ (meaning that $x_i \leftarrow x_i+\Delta$); in the end, with probability at least $1-\delta$, it gives an ``$\ell_p$-sample'' according to $x$: namely, item $i$ is sampled with probability $\frac{|x_i|^p}{\sum_{j\in [n]}{|x_j|^p}}$. 

Note that updates are independent of the randomness used in the sampler. That is, for the purpose of proving a lower bound, we assume an oblivious adversary. 

To the best of our knowledge, the best space upper bound for $\ell_0$ sampler is $O(\log^2 n \log \frac{1}{\delta})$ bits, while the previous best lower bound is $\Omega(\log^2 n +\log\frac{1}{\delta})$ bits (where $\Omega(\log^2 n)$ is shown in \cite{JowhariST11}). The bound is tight for constant $\delta$, while for example, when $\delta=\frac{1}{n}$, the gap is $\log n$. 

We show space lower bounds for maintaining a sampler for a {\em binary} vector. That is, at any time, we are guaranteed that $x\in \{0,1\}^n$. This makes our result strong in the sense that (1) the lower bound applies for any $p$; (2) the lower bound also works for strict turnstile streams.

The lower bounds are based on communication complexity in the public random coin model. Alice wants to send Bob a uniform random set $A\subseteq [n]$ of size $m$ (Bob knows $m$, but the random source generating $A$ is independent of the random source accessible to Bob). The one-way communication problem is: Alice sends some message to Bob, and Bob is required to recover $A$ completely. Since the randomness in $A$ contains $\log (^n_m)$ bits of information, any randomized protocol that works with probability $1$ requires at least $\log (^n_m)$ expected bits. 

We consider the following protocol. Alice attaches (the memory of) a sampler \samp in the message. The sampler uses public random coins as its random source, so that the sampler will behave the same at Alice's and Bob's as long as the updates are all the same. Alice will insert all the items in $A$ into \samp and send \samp to Bob. In addition, Alice will send a subset $B\subseteq A$ to Bob, so that together with $B$ and \samp, Bob is able to recover $A$ with good probability based on some protocol they have agreed on. 
 
Now we turn the previous protocol into a new one without any failure. Let \success denote the event (or a subset of the event) that Bob successfully recovers $A$ (note that Alice can simulate Bob, so she knows exactly when \success happens). If \success happens Alice will send Bob a message starting with a $1$, followed by (the memory of) \samp, then followed by the native encoding (explained later) of $B$; otherwise, Alice will send a message starting with a $0$, followed by the native encoding of $A$. We say the native encoding of a set $S\subseteq [n]$ to be an integer (expressed in binary) in $[{n \choose |S|}]$ together with $|S|$ (taking $\log n$ bits). We drop the size of the set if it is known by the receiver.

\begin{lemma} \label{lemma:lb-meta}
  Let $\s$ denote the space (in bits) used by a sampler with failure probability at most $\delta$. Let $\s'$ denote the expected number of bits to represent $B$ conditioned on \success (if we need to send some extra auxiliary information, we will also count it into $\s'$). We have 
  
  \begin{align}
  (1+\s+\s')\cdot \Pr(\success)+(1+\log (^n_m)) \cdot (1-\Pr(\success)) \ge \log (^n_m).
  \end{align} 
  
  If $\Pr(\success)\ge 1/2$, we have 
  
  \begin{align} \label{formula:lb-meta}
  \s\ge \log (^n_m) - \s' - 2.
  \end{align} 
\end{lemma}

We consider the range of failure probability $\delta$ to be 
\begin{align} \label{formula:delta-range}
2^{-n^{c_1}}<\delta<c_2,
\end{align}
where $c_1=0.9$ and $c_2=2^{-10}$. In fact, our lower bound applies for the range of $\delta$ where $c_1$ and $c_2$ are any constants smaller than $1$. 

In Section~\ref{sec:simple-lb} we give a lower bound of $\Omega(\log n \log \frac{1}{\delta})$ bits. This illustrates some key ideas of our framework. Then we show a lower bound of $\Omega(\log^2 n \log \frac{1}{\delta})$ bits in Section~\ref{sec:optimal-lb}.

\begin{remark}
  Because the space lower bound in this note is proven via communication complexity under public random coin model, it also applies to non-uniform models of computation such as circuits and branching programs.  
\end{remark}

\begin{remark}
  The space lower bound in this note still applies if the sampler is required to output an arbitrary item whose coordinate is non-zero instead of a uniformly random one. 
\end{remark}

%=====================================================================
\section{$\Omega(\log n \log {\frac{1}{\delta}})$ Bits Lower Bound}\label{sec:simple-lb}
Let $m=\frac{1}{2}\log \frac{1}{\delta}$, namely, Alice wants to send a uniform random set $A\subseteq [n]$ of size $\frac{1}{2}\log\frac{1}{\delta}$ to Bob. Let $A=\{a_1,\ldots,a_m\}$ and $a_1<\ldots<a_m$. 

\begin{algorithm}[H]
\caption{Alice's Encoder.}
\begin{algorithmic}[1]
\Procedure{$\enc_1$}{$A$}
  \State $\samp \leftarrow \emptyset$ \Comment{\samp uses the random source shared by Alice and Bob}
  \For {$i=1,2,\ldots,m$}
    \State Insert $a_i$ into \samp
  \EndFor
  \State \Return \samp 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Bob's Decoder.}
\begin{algorithmic}[1]
\Procedure{$\dec_1$}{\samp}
  \State $S\leftarrow \emptyset$
  \For {$i=1,2,\ldots,m$}
    \State Let $\samp_i$ be a copy of \samp \Comment{So that $\samp_i$ can behave as if it is \samp}
    \For {$s \in S$} \Comment{Enumerate the elements in $S$ from smallest to biggest}
      \State Remove $s$ from $\samp_i$
    \EndFor
    \State Obtain a sample $s_i$ from $\samp_i$
    \State $S \leftarrow S \cup \{s_i\}$
  \EndFor
  \State \Return $S$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
  For any $A\subseteq [n]$, where $|A|=m=\frac{1}{2}\log \frac{1}{\delta}$, $\Pr(\dec_1(\enc_1(A))=A)\ge 1/2$.
\end{lemma}

\begin{proof}
  Let $E_S$ denote the event that after removing all the items in $S$ (in the order from smallest to biggest) from \samp, it gives a valid sample when queried. We have 
  
  \begin{align}
  \Pr(\dec_1(\enc_1(A))=A)
  \ge \Pr(\bigcap_{S\subseteq A, S\ne \emptyset}{E_S})
  \ge 1 - \sum_{S\subseteq A, S\ne \emptyset}{\Pr({\overline {E_S}})} 
  \ge 1 - \delta \cdot 2^{\frac{1}{2}\log \frac{1}{\delta}}
  \ge 1/2.
  \end{align}
\end{proof}

\begin{lemma}
  $\s = \Omega(\log n \log \frac{1}{\delta})$.
\end{lemma}

\begin{proof}
  It follows from Formula~\ref{formula:lb-meta} in Lemma~\ref{lemma:lb-meta}, where $\log {n \choose \frac{1}{2}\log \frac{1}{\delta}}=\Omega(\log n \log \frac{1}{\delta})$ and $\s'=0$. 
\end{proof}

\begin{remark}
  The following decoder $\dec_1'$ is similar to $\dec_1$, but we will lose a factor of $\log\log \frac{1}{\delta}$ in the lower bound because by doing so we have to union-bound $O(m!)$ events instead of $O(2^m)$ events (so that in turn we have to set $m$ to be $\frac{\log \frac{1}{\delta}}{\log\log\frac{1}{\delta}}$ in order to have good success probability). 
\end{remark}

\begin{algorithm}[H]
\caption{A Worse Decoder.}
\begin{algorithmic}[1]
\Procedure{$\dec_1'$}{\samp}
\State $S\leftarrow \emptyset$
\For {$i=1,2,\ldots,m$}
\State Obtain a sample $s_i$ from $\samp$
\State Remove $s_i$ from $\samp$
\State $S \leftarrow S \cup \{s_i\}$
\EndFor
\State \Return $S$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}


%=====================================================================
\section{$\Omega(\log^2 n \log \frac{1}{\delta})$ Bits Lower Bound} \label{sec:optimal-lb}

In the previous section we have shown how to extract $\Theta(\log \frac{1}{\delta})$ words of information from a sampler. 
Our goal is to extract more words. New observation comes from the upper bound for constructing an $\ell_0$ sampler. 
When $\delta=\frac{1}{n}$, we have the following sampler algorithm that consumes $O(\log^3 n)$ bits. 
The sampler consists of $\log n$ layers, and on layer $i=1,\ldots, \log n$ it maintains a separate $\log n$-sparse recover system for the sub-stream generated by sub-sampling the items from the universe $[n]$ with probability $2^{-i}$.
Each sparse recovery system takes $O(\log^2 n)$ bits. 
Its correctness comes from the fact that 
\begin{enumerate}
  \item With probability at least $1-n^{-c}$ there is some layer $i$ that contains at least $1$ and at most $\log n$ items whose coordinates are non-zero. 
  \item Conditioned on the event that on layer $i$ the number of items whose coordinates are non-zero is between $1$ and $\log n$, the sparse recovery system on layer $i$ works with failure probability at most $n^{-c}$. In this context, we say the sparse recovery system works if it could recover at least one item. 
\end{enumerate}

Intuitively, the previous lower bound only extracts information from one single layer (i.e. layer $0$). In this section, we build a framework to extract information from multiple layers. A second technique that makes our improvement possible is random removal: after obtaining a sample from the sampler, we not only peel off the sample, but also randomly remove a fraction of items remaining in the sampler. 

\subsection{Protocol}

The parameters used by Alice and Bob are given in Algorithm~\ref{algo:para}.
Alice wants to send a uniformly random set $A$ to Bob where $|A|=m$. Similar to $\enc_1$, Alice constructs \samp by inserting all the items in $A$, and sends it to Bob. Note that \samp uses the same random source that Alice and Bob share. Moreover, Alice will send Bob a subset $B\subseteq A$ computed as follows. Initially $B=A$ and let $A_0=A$. Alice proceeds in $R$ rounds. On round $r$ ($r=1,\ldots, R$) Alice tries to obtain sample $s_r\in A_{r-1}$ from the sampler 
(In more detail, she will make a copy of \samp, denoted by $\samp_r$, remove all the items in $A\backslash A_{r-1}$ and query for a sample). 
Let $b$ denote a binary string of length $R$, where $b_r$ records whether the sampler succeeds on round $r$. In the end, Alice will also send $b$ to Bob. If $s_r\in A_{r-1}$, i.e., the sampler returns a valid sample, Alice will set $b_r=1$, and remove $s_r$ from $B$ (since $s_r$ can be obtained from the sampler, Alice does not need to include it in $B$. In Algorithm~\ref{algo:enc} Alice uses $S$ to keep track of $A\backslash B$); otherwise Alice will set $b_r=0$.
At the end of round $r$, Alice will generate a uniformly random set $A_r$, so that $A_r$ is a subset of $A_{r-1}\backslash \{s_r\}$ and $|A_r|=n_r$. 
In particular, Alice uses shared random source to generate $A_r$, so that Bob can recover $A_{r-1}\backslash A_r$ on round $r$. 
We present Alice's encoder in Algorithm~\ref{algo:enc}.

The decoding process is symmetric. Let $C_0=\emptyset$ and $S=\emptyset$. Bob proceeds in $R$ rounds. 
On round $r$ ($r=1,\ldots,R$), Bob obtains sample $s_r\in A\backslash C_{r-1}$ (In more detail, he will make a copy of \samp, denoted by $\samp_r$, remove all the items in $C_{r-1}$ and query for a sample). 
By the construction of $C_{r-1}$ that will be described later it is guaranteed that $A_{r-1}=A\backslash C_{r-1}$. 
Therefore, Bob will get exactly the same $s_r$ as Alice. 
Bob assigns initial value $C_{r-1}$ to $C_r$.
If $b_r=1$, Bob will add $s_r$ to both $S$ and $C_r$.
At the end of round $r$, Bob inserts a bunch of items to $C_r$ so that $C_r=A\backslash A_r$. Bob can achieve this because of the shared randomness when constructing $A_r$.
In the end, Bob's decoder outputs $B\cup S$.
We present the decoder in Algorithm~\ref{algo:dec}.

\begin{algorithm}[H] 
  \caption{Variables Shared by Alice's $\enc$ and Bob's $\dec$.} \label{algo:para}
  \begin{algorithmic}[1] 
    \State $m\leftarrow n^{0.99}$
    \State $K\leftarrow \frac{1}{10}\log \frac{1}{\delta}$
    \State $R\leftarrow \frac{1}{200}\log n \log \frac{1}{\delta}$
    \For {$r = 0, \ldots, R$}
      \State $n_r\leftarrow m \cdot 2^{-\frac{r}{K}}$ \Comment{By the setting of parameters we have $n_r-n_{r+1}\ge 2$}
    \EndFor
    \For {$a\in [n]$}
      \State Let $U_a$ be uniformly and independent sample from $[0,1]$ \Comment{Used to generate $A_r$, $r=1,\ldots, R$}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Alice's Encoder.} \label{algo:enc}
  \begin{algorithmic}[1]
    \Procedure{$\enc$}{$A$}
    \State $\samp \leftarrow \emptyset$
    \State Insert items in $A$ into \samp
    \State $A_0 \leftarrow A$
    \State $S\leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
      \State Let $\samp_r$ be a copy of $\samp$
      \State Remove items in $A\backslash A_{r-1}$ from $\samp_r$ \Comment So that $\samp_r$ contains the elements in $A_{r-1}$
      \State Let $s_r$ be a sample from $\samp_r$
      \State $A_r\leftarrow A_{r-1}$
      \If {$s_r\in A_r$} \Comment{i.e. if $s_r$ is a valid sample}
        \State $b_r\leftarrow 1$ \Comment{$b$ is a binary string of length $R$, indicating if sampler succeeds on round $r$}
        \State $S\leftarrow S \cup \{s_r\}$
        \State $A_r\leftarrow A_r \backslash \{s_r\}$
      \Else 
        \State $b_r\leftarrow 0$
      \EndIf
      \State Remove $|A_r|-n_r$ elements from $A_r$ with smallest $U_a$'s among $a\in A_r$ \Comment{So that $|A_r|=n_r$}
    \EndFor
    \State \Return ($A\backslash S$, $b$, \samp) 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Bob's Decoder.} \label{algo:dec}
  \begin{algorithmic}[1]
    \Procedure{$\dec$}{$B$, $b$, \samp}
    \State $S\leftarrow \emptyset$
    \State $C_0 \leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
      \State $C_r\leftarrow C_{r-1}$
      \If{$b_r=1$}
        \State Let $\samp_r$ be a copy of $\samp$
        \State Remove items in $C_{r-1}$ from $\samp_r$ \Comment{Invariant: $C_r=A\backslash A_r$ ($A_r$ is defined in $\enc$)}
        \State Let $s_r$ be a sample from $\samp_r$
        \State $S\leftarrow S \cup \{s_r\}$
        \State $C_r\leftarrow C_r \cup \{s_r\}$
      \EndIf
       \State Insert $m-n_r-|C_r|$ items into $C_r$ with smallest $U_a$'s among $a\in B\backslash C_r$
    \EndFor
    \State \Return $B\cup S$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Analysis}

We have three random objects here: (1) Set $A$, which is a uniform random set of size $m$; (2) The random source used by the sampler, denoted by $X$; (3) The collection of random variables $U_a$ where $a\in [n]$, denoted by $U$. They are independent. We do the analysis conditioned on $A$. Namely, the following arguments apply for any fixed $A$. 

First, we can prove that $\dec(\enc(A))=A$. That is, no matter the randomness in $X$ and $U$, Bob will always decode $A$ successfully. It is because Alice's $\enc$ and Bob's $\dec$ share $X$ and $U$, so that Bob essentially simulates Alice. We formally prove this by induction in Lemma~\ref{lemma:zero-fail-prob}. 

Now our goal is to prove that by using \samp, the number of bits that Alice saves is $\Omega(\log^2 n \log\frac{1}{\delta})$. Intuitively, it is equivalent to prove the number of words that Alice saves is $\Omega(\log n \log\frac{1}{\delta})$.
We formalize this in Lemma~\ref{lemma:bits-saving}. Note that Alice also needs to send $b$ (i.e., whether the sampler succeeds on $R$ rounds) to Bob, but it takes only $O(\log n \log\frac{1}{\delta})$ bits, which is negligible. Thus it is sufficient to prove $\E(|B|)=|A|-\Omega(\log n \log\frac{1}{\delta})$. 

We have $|A|-|B|=\sum_{r=1}^{R}b_r$. 
In Lemma~\ref{lemma:mutual-entropy-vs-fail-prob}, we prove the probability that the sampler fails on round $r$ is upper bounded by $\frac{I(X;A_{r-1})+1}{\log \frac{1}{\delta}}$, where $I(X;A_{r-1})$ is the mutual information between $X$ and $A_{r-1}$. 
Furthermore, we will show in Lemma~\ref{lemma:mutual-entropy-bound} that $I(X;A_{r-1})$ is upper bounded by $5K$.
Combining together we get $\E(b_r)\ge 1-\frac{5K+1}{\log \frac{1}{\delta}}\ge \frac{2}{5}$ and thus $\E(|A|-|B|)\ge \frac{2}{5}R=\Omega(\log n \log\frac{1}{\delta})$. 

\begin{lemma}\label{lemma:zero-fail-prob}
  $\dec(\enc(A))=A$.
\end{lemma}

\begin{proof}
  We claim that for $r=0,\ldots, R$, $\{A_r, C_r\}$ is a partition of $A$ ($A_r$ is in \enc, and $C_r$ is in \dec). We prove the claim by induction on $r$.
  
  The basis case is $r=0$. $A_0=A$ and $C_0=\emptyset$. The claim is true.
  
  Assume by induction the claim holds for $r$ ($0\le r < R$), and we consider $r+1$. 
  On round $r$, $\samp_r$ is \samp with items in $A\backslash A_{r-1}=C_{r-1}$ removed in both \enc and \dec. Therefore, the sample $s_r$ obtained from $\samp_r$ in both sides is the same. Initially $A_r=A_{r-1}$ and $C_r=C_{r-1}$, and so $\{A_r,C_r\}$ is a partition of $A$. If $s_r$ is a valid sample (i.e. $s_r\in A_{r-1}$), then $b_r=1$, and \enc removes $s_r$ from $A_r$ and in the meanwhile \dec inserts $s_r$ into $C_r$, so that $\{A_r, C_r\}$ remains a partition of $A$. 
  
  After that, Alice's \enc repeats removing $a$ from $A_r$ with smallest $U_a$ until $|A_r|=n_r$. Symmetrically, Bob's \dec repeats inserting $a$ into $C_r$ with smallest $U_a$ among $a\in B\backslash C_r$, until $|C_r|=|A|-n_r$. In the end we have $|A_r|+|C_r|=|A|$, so \enc and \dec execute repetition the same number of times. Moreover, we can prove that during the same repetition the item removed from $A_r$ is exactly the same item inserted to $C_r$. This is because in the beginning of a repetition $\{A_r, C_r\}$ is a partition of $A$. We have $B\backslash C_r\subseteq A\backslash C_r=A_r$. Let $a^*$ denote  $a\in A_r$ that minimizes $U_a$. We can prove that $a^*\in B\backslash C_r\subseteq A_r$ (since $a^*$ will be removed from $A_r$, it has no chance to be included in $S$ in \enc, so that $B$ contains $a^*$), and $U_{a^*}$ is also the smallest among $\{U_a|a\in B\backslash C_r\}$. Thus in the end of the repetition, both Alice and Bob will take $a^{*}$ to update (remove from $A_r$, insert into $C_r$). Therefore, $\{A_r, C_r\}$ remains a partition of $A$.
  
  Given the fact that $\{A_r, C_r\}$ is a partition of $A$, $s_r$ is the same in \enc and \dec. Furthermore, $S=\{s_r|b_r=1,r=1,\ldots, R\}$ is the same in \enc and \dec. We know $S\subseteq A$. Since \enc outputs $A\backslash S$, and \dec outputs $(A\backslash S)\cup S$, we have $\dec(\enc(A))=A$. 
\end{proof}

\begin{lemma} \label{lemma:bits-saving}
  Let $m=n^{0.99}$. Let $W\in \mathbb{N}$ be a random variable, and $W\le m$. Moreover, $\E(W)\le m-d$. We have $\E(\log {n \choose m}-\log {n \choose W})=\Omega(d \log n)$.
\end{lemma}

\begin{proof}
  \begin{align}
  \log {n \choose m}-\log {n \choose W}
  &= \log \frac{n!/(m!(n-m)!)}{n!/(W!(n-W)!)} \\
  &= \sum_{i=1}^{m-W}\log \frac{n-W-i+1}{m-i+1} \\
  &\ge (m-W)\cdot \log \frac{n-W}{m} \\
  &\ge (m-W)\cdot \log n^{1/200}
  \end{align}
  
  Taking expectation on both sides, we get $\E(\log {n \choose m}-\log {n \choose W})\ge \frac{d}{200} \log n$. 
\end{proof}

\begin{lemma}\label{lemma:mutual-entropy-vs-fail-prob}
  Let function $f$: $\{0,1\}^n\times \{0,1\}^m\rightarrow \{0,1\}$. Let $X$ be a uniformly random string in $\{0,1\}^n$. If for any $y\in \{0,1\}^m$ we have $\Pr(f(X,y)=1)\le \delta$ where $0<\delta<1$, then for any random variable $Y$ in $\{0,1\}^m$, we have 
  \begin{align}
    \Pr(f(X,Y)=1)\le \frac{I(X;Y)+1}{\log \frac{1}{\delta}},
  \end{align}
  where $I(X;Y)$ is the mutual information (in bits) between $X$ and $Y$.
\end{lemma}

\begin{proof}
  It is equivalent to prove $I(X;Y)\ge \E(f(X,Y))\cdot \log\frac{1}{\delta}-1$. By definition of mutual entropy, $I(X;Y)=H(X)-H(X|Y)$ where $H(X)=n$ and $H(X|Y)\le 1+(1-\E(f(X,Y)))\cdot n+\E(f(X,Y))\cdot (n-\log\frac{1}{\delta})=n+1-\E(f(X,Y))\cdot \log\frac{1}{\delta}$.
  The upper bound for $H(X|Y)$ is obtained by considering the following one-way communication problem: Alice obtains both $X$ and $Y$ while Bob only gets $Y$, what is the (minimum) expected number of bits that Alice sends to Bob so that Bob can recover $X$? 
  Any protocol gives an upper bound for $H(X|Y)$, and we simply take the following protocol: first Alice sends Bob $f(X,Y)$ (taking $1$ bit); and then if $f(X,Y)=0$ Alice sends $X$ directly (taking $n$ bits), otherwise, $f(X,Y)=1$, Alice sends the index of $X$ in $\{x|f(x,Y)=1\}$ (taking $\log (\delta 2^n)=n-\log\frac{1}{\delta}$ bits).  
\end{proof}

\begin{corollary}\label{corollary:sampler-failure}
  Given a sampler with failure probability at most $\delta$ whenever the updates to it are independent from the randomness used by the sampler. Let $X$ denote the random source used by the sampler, and let $D$ denote the random source generating the updates. Then the probability that the sampler fails on updates $D$ is at most $\frac{I(X;D)+1}{\log\frac{1}{\delta}}$.
\end{corollary}

\begin{lemma}\label{lemma:mutual-entropy-bound}
  $I(X;A_r)\le 5K$, for $r=1,\ldots, R$.
\end{lemma}

\begin{proof}
  $I(X;A_r)=H(A_r)-H(A_r|X)$. Since $|A_r|=n_r$ and $A_r\subseteq A$ where $|A|=m$, $H(A_r)\le \log {m \choose n_r}$. Now we want to lower bound $H(A_r|X)$. By definition of conditional entropy, $H(A_r|X)=\sum_x{p_x\cdot H(A_r|X=x)}$. We fix an arbitrary $x$. If we can prove that for any $T\subseteq A$ where $|T|=n_r$, $\Pr(A_r=T|X=x)\le p$, then by definition of entropy we have $H(A_r|X=x)\ge\log\frac{1}{p}$. In fact, for any fixed $T$, we have
  
  \begin{align}
    \Pr(A_r=T|X=x)\le \prod_{i=1}^{r}{\frac{{n_{i-1}-n_r-1 \choose n_{i-1}-n_i-1}}{{n_{i-1}-1 \choose n_{i-1}-n_i-1}}},
  \end{align}
  
  because on round $i$ ($1\le i \le r$), Alice removes $n_{i-1}-n_i$ elements from $A_{i-1}$ to get $A_i$. Conditioned on the event that $A_{i-1}\supseteq T$, the probability that $A_i\supseteq T$ is at most ${{n_{i-1}-n_r-1 \choose n_{i-1}-n_i-1}}/{{n_{i-1}-1 \choose n_{i-1}-n_i-1}}$, where the equation achieves when $s_i\in A_{i-1}\backslash T$, and Alice takes a uniformly random subset of $A_{i-1}\backslash \{s_i\}$ of size $n_{i-1}-n_i-1$, so that the subset does not intersect with $T$.
  
  For notation simplicity, let $n^{\underline{k}}$ denote $n\cdot (n-1)\ldots (n-k+1)$. We have 
  \begin{align}
    \prod_{i=1}^{r}{\frac{{n_{i-1}-n_r-1 \choose n_{i-1}-n_i-1}}{{n_{i-1}-1 \choose n_{i-1}-n_i-1}}}
    =\prod_{i=1}^{r}\frac{(n_{i-1}-n_r-1)!n_i!}{(n_{i-1}-1)!(n_i-n_r)!}
    =\prod_{i=1}^{r}\frac{n_i^{\underline{n_r}}}{(n_{i-1}-1)^{\underline{n_r}}}
    =\prod_{i=1}^{r} \left( \frac{n_i^{\underline{n_r}}}{n_{i-1}^{\underline{n_r}}}\cdot \frac{n_{i-1}}{n_{i-1}-n_r} \right).
  \end{align}
  
  By telescoping,
  \begin{align}
    \prod_{i=1}^{r} \frac{n_i^{\underline{n_r}}}{n_{i-1}^{\underline{n_r}}}
    =\frac{n_r^{\underline{n_r}}}{n_0^{\underline{n_r}}}
    =\frac{n_r!(n_0-n_r)!}{n_0!}=\frac{1}{{n_0 \choose n_r}}
    =\frac{1}{{m \choose n_r}}.
  \end{align}
  
  Moreover, 
  \begin{align}
    \prod_{i=1}^{r} \frac{n_{i-1}}{n_{i-1}-n_r}
    =\prod_{i=1}^{r} \frac{1}{1-2^{(i-1-r)/K}}
    =\prod_{j=1}^{r} \frac{1}{1-2^{-j/K}}
    \le \prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}.
  \end{align}
  
  By Lemma~\ref{lemma:Pochhammer}, we have $\prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}\le 2^{5K}$. Let $p={2^{5K}}/{{m\choose n_r}}$, we have $\Pr(A_r=T|X=x)\le p$ and thus $H(A_r|X=x)\ge \log\frac{1}{p}=\log{{m\choose n_r}}-5K$. Therefore, $H(A_r|X)\ge \log{{m\choose n_r}}-5K$ and so $I(X;A_r)=H(A_r)-H(A_r|X)\le 5K$.  
\end{proof}

%By \url{http://mathworld.wolfram.com/q-PochhammerSymbol.html} We have $\prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}\le 2^{5K}$. I think we can improve the constant 5 to 4 if we bound it more carefully. 
\begin{lemma}\label{lemma:Pochhammer}
  Let $K\in \mathbb{N}$ and $K\ge 1$. We have $\prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}\le 2^{5K}$.
\end{lemma}

\begin{proof}
  First, we bound the product of first $2K$ terms. Note that $\frac{1}{1-2^{-x}}\le \frac{8}{3x}$ for $0<x\le 2$. Therefore, 
  \begin{align}
    \prod_{j=1}^{2K}\frac{1}{1-2^{-j/K}}
    \le (8/3)^{2K}\cdot \frac{K^{2K}}{(2K)!}
    \le (8/3)^{2K}\cdot \frac{K^{2K}}{(2K/e)^{2K}}
    = (4e/3)^{2K}
    < 2^{4K}. 
  \end{align}
  
  Then, we bound the product of the rest terms
  \begin{align}
    \prod_{j=2K+1}^{\infty}\frac{1}{1-2^{-j/K}} 
    \le \prod_{j=2K+1}^{\infty}\frac{1}{1-2^{-\lfloor j/K \rfloor}} 
    \le \prod_{i=2}^{\infty}\left( \frac{1}{1-2^{-i}}\right)^K 
    \le \left( \frac{1}{1-\sum_{i=2}^{\infty}2^{-i}}\right)^K
    = 2^K.
  \end{align}
  
  Multiplying two parts proves the lemma.
\end{proof}

\begin{theorem}
  $\s = \Omega(\log^2 n\log{\frac{1}{\delta}})$.
\end{theorem}

\begin{proof}
  By Lemma~\ref{lemma:zero-fail-prob}, the success probability of protocol $(\enc,\dec)$ is $1$. 
  By Lemma~\ref{lemma:lb-meta}, we have $\s\ge \log (^n_m) - \s' -2$, where $\s'=\log n + R+ \E(\log (^n_{|B|}))$. 
  The size of $B$ is $|B|=|A|-\sum_{r=1}^{R}{b_r}$.
  By Corollary~\ref{corollary:sampler-failure}, and because conditioned on $A$, the randomness of updates to $\samp_r$ comes from $A_{r-1}$ (for $r=1, \ldots, R$), we get $\Pr(b_r=0)\le \frac{I(X;A_{r-1})+1}{\log\frac{1}{\delta}}$ and thus $\E(b_r)\ge 1-\frac{I(X;A_{r-1})+1}{\log\frac{1}{\delta}}$. 
  By Lemma~\ref{lemma:mutual-entropy-bound}, $I(X;A_{r-1})\le 5K$ (Note that when $r=1$, $I(X;A_0)=0\le 5K$). Therefore, $\E(b_r)\ge 1-\frac{5K+1}{\log\frac{1}{\delta}}$.
  By the setting of parameters (see Algorithm~\ref{algo:para}) we have $\E(b_r)\ge \frac{2}{5}$. Therefore, $\E(|B|)\le |A|-\frac{2}{5}R$. 
  By Lemma~\ref{lemma:bits-saving}, $\E(\log (^n_{|B|}))=\log (^n_m)-\Omega(R\log n)$. 
  Combining together we get $\s=\Omega(R\log n)=\Omega(\log^2 n \log\frac{1}{\delta})$.
\end{proof}

\bibliographystyle{alpha}
\bibliography{shortbib}

\end{document}
























