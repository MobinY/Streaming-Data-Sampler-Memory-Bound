\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx} 
\usepackage{filecontents}
\usepackage[usenames]{color}
\usepackage{algorithm,algpseudocode,float}
\usepackage{xspace}

\newcommand{\EDIT}[1]{\textcolor{red}{\textit{#1}}}

\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\DeclareMathOperator*{\eps}{\varepsilon}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}

\usepackage{enumerate}

\newcommand{\samp}{\textsf{SAMP}\xspace}
\newcommand{\success}{\textsf{SUCC}\xspace}
\newcommand{\enc}{\textsf{ENC}\xspace}
\newcommand{\dec}{\textsf{DEC}\xspace}
\newcommand{\s}{\textsf{s}\xspace}

\title{A Note on Space Lower Bound for Samplers}
\author{Jelani Nelson, Jakub Pachocki, Zhengyu Wang}

\begin{document}
	
\maketitle

We study the space lower bound for maintaining a sampler over a turnstile stream. An $\ell_p$-sampler with failure probability at most $\delta$ is a randomized data structure for maintaining vector $x\in \mathbb{R}^n$ (initially 0) under a stream of updates in the form of $(i, \Delta)$ (meaning that $x_i \leftarrow x_i+\Delta$); in the end, with probability at least $1-\delta$, it gives an ``$\ell_p$-sample'' according to $x$: namely, item $i$ is sampled with probability $\frac{|x_i|^p}{\sum_{j\in [n]}{|x_j|^p}}$. 

Note that updates are independent of the randomness used in the sampler. That is, for the purpose of proving a lower bound, we assume an oblivious adversary. 

To the best of my knowledge, the best space upper bound for $\ell_0$ sampler is $O(\log^2 n \log \frac{1}{\delta})$ bits, while the previous best lower bound is $\Omega(\log^2 n +\log\frac{1}{\delta})$ bits (where $\Omega(\log^2 n)$ is shown in \cite{jowhari2011tight}). The bound is tight for constant $\delta$, while for example, when $\delta=\frac{1}{n}$, the gap is $\log n$. 

We assume that 
\begin{align} \label{formula:delta-range}
2^{-n^{c_1}}<\delta<(\log n)^{-c_2},
\end{align}
where $c_1=0.01$ and $c_2=2$. For other range of $\delta$ we will study later.

In this note, we show space lower bounds for maintaining a sampler for a {\em binary} vector. That is, at any time, we are guaranteed that $x\in \{0,1\}^n$. This makes our result strong in the sense that (1) the lower bound applies for any $p$; (2) the lower bound also works for strict turnstile streams.

In the following sections, we give sequentially improved lower bounds. First, we give a lower bound of $\Omega(\log n \log \frac{1}{\delta})$ bits. Then, we improve it to $\Omega(\frac{\log^2 n \log \frac{1}{\delta}}{(\log\log n + \log\log \frac{1}{\delta})^2})$ bits. The lower bounds are based on communication complexity in the public random coin model. Alice wants to send Bob a uniform random set $A\subseteq [n]$ of size $m$ (Bob knows $m$, but the random source generating $A$ is independent of the random source accessible to Bob). The one-way communication problem is: Alice sends some message to Bob, and Bob is required to recover $A$ completely. Since the randomness in $A$ contains $\log (^n_m)$ bits of information, any randomized protocol that works with probability $1$ requires at least $\log (^n_m)$ expected bits. 

Now Alice considers to attach (the memory of) a sampler \samp in the message. The sampler uses public random coins as its random source, so that the sampler will behave the same at Alice's and Bob's as long as the updates are all the same. Alice will insert all the items in $A$ into \samp and send \samp to Bob. In addition, Alice will send a subset $B\subseteq A$ to Bob, so that together with $B$ and \samp, Bob is able to recover $A$ with good probability based on some protocol they have agreed on. 
 
Now we turn the previous protocol into a new one without any failure. Let \success denote the event (or a subset of the event) that Bob successfully recovers $A$ (note that Alice can simulate Bob, so she knows exactly when \success happens). If \success happens Alice will send Bob a message starting with a $1$, followed by (the memory of) \samp, then followed by the native encoding (explained later) of $B$; otherwise, Alice will send a message starting with a $0$, followed by the native encoding of $A$. We say the native encoding of a set $S\subseteq [n]$ to be an integer (expressed in binary) in $[{n \choose |S|}]$ together with $|S|$ (taking $\log n$ bits). We drop the size of the set if it is known by the receiver.

\begin{lemma} \label{lemma:lb-meta}
  Let $\s$ denote the space (in bits) used by a sampler with failure probability at most $\delta$. Let $\s'$ denote the expected number of bits to represent $B$ conditioned on \success (if we need to send some extra auxiliary information, we will also count it into $\s'$). We have 
  
  $$(1+\s+\s')\cdot \Pr(\success)+(1+\log (^n_m)) \cdot (1-\Pr(\success)) \ge \log (^n_m).$$
  
  If $\Pr(\success)\ge 1/2$, we have 
  
  \begin{align} \label{formula:lb-meta}
  \s\ge \log (^n_m) - \s' - 2.
  \end{align} 
\end{lemma}

\begin{remark}
  Because the space lower bound in this note is proven via communication complexity under public random coin model, it also applies to non-uniform models of computation such as circuits and branching programs.  
\end{remark}


\begin{remark}
  The space lower bound in this note still applies if the sampler is required to output an arbitrary item whose coordinate is non-zero instead of a uniformly random one. 
\end{remark}

%=====================================================================
\section{$\Omega(\log n \log {\frac{1}{\delta}})$ Bits Lower Bound}
Let $m=\frac{1}{2}\log \frac{1}{\delta}$, namely, Alice wants to send a uniform random set $A\subseteq [n]$ of size $\frac{1}{2}\log\frac{1}{\delta}$ to Bob. Let $A=\{a_1,\ldots,a_m\}$ and $a_1<\ldots<a_m$. 

\begin{algorithm}[H]
\caption{Alice's Encoder.}
\begin{algorithmic}[1]
\Procedure{$\enc_1$}{$A$}
  \State $\samp \leftarrow \emptyset$
  \For {$i=1,2,\ldots,m$}
    \State Insert $a_i$ into \samp
  \EndFor
  \State \Return \samp 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Bob's Decoder.}
\begin{algorithmic}[1]
\Procedure{$\dec_1$}{\samp}
  \State $S\leftarrow \emptyset$
  \For {$i=1,2,\ldots,m$}
    \State Let $\samp_i$ be a copy of \samp \Comment{So that $\samp_i$ can behave as if it is \samp}
    \For {$s \in S$} \Comment{Enumerate the elements in $S$ from smallest to biggest}
      \State Remove $s$ from $\samp_i$
    \EndFor
    \State Obtain a sample $s_i$ from $\samp_i$
    \State $S \leftarrow S \cup \{s_i\}$
  \EndFor
  \State \Return $S$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
  For any $A\subseteq [n]$, where $|A|=m=\frac{1}{2}\log \frac{1}{\delta}$, $\Pr(\dec_1(\enc_1(A))=A)\ge 1/2$.
\end{lemma}

\begin{proof}
  Let $E_S$ denote the event that after removing all the items in $S$ (in the order from smallest to biggest) from \samp, it gives a valid sample when queried. We have 
  
  $$\Pr(\dec_1(\enc_1(A))=A)
  \ge \Pr(\bigcap_{S\subseteq A, S\ne \emptyset}{E_S})
  \ge 1 - \sum_{S\subseteq A, S\ne \emptyset}{\Pr({\overline {E_S}})} 
  \ge 1 - \delta \cdot 2^{\frac{1}{2}\log \frac{1}{\delta}}
  \ge 1/2.
  $$  
\end{proof}

\begin{lemma}
  $\s = \Omega(\log n \log \frac{1}{\delta})$ for $2^{-n^{0.99}}<\delta<\frac{1}{4}$.
\end{lemma}

\begin{proof}
  It follows from Formula~\ref{formula:lb-meta} in Lemma~\ref{lemma:lb-meta}, where $\log {n \choose \frac{1}{2}\log \frac{1}{\delta}}=\Omega(\log n \log \frac{1}{\delta})$ and $\s'=0$. 
\end{proof}

\begin{remark}
  The following decoder $\dec_1'$ is similar to $\dec_1$, but we will lose a factor of $\log\log \frac{1}{\delta}$ in the lower bound because by doing so we have to union-bound $O(m!)$ events instead of $O(2^m)$ events (so that in turn we have to set $m$ to be $\frac{\log \frac{1}{\delta}}{\log\log\frac{1}{\delta}}$ in order to have good success probability). 
\end{remark}

\begin{algorithm}[H]
\caption{A Worse Decoder.}
\begin{algorithmic}[1]
\Procedure{$\dec_1'$}{\samp}
\State $S\leftarrow \emptyset$
\For {$i=1,2,\ldots,m$}
\State Obtain a sample $s_i$ from $\samp$
\State Remove $s_i$ from $\samp$
\State $S \leftarrow S \cup \{s_i\}$
\EndFor
\State \Return $S$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}


%=====================================================================
\section{$\Omega(\frac{\log^2 n\log{\frac{1}{\delta}}}{(\log\log n+\log\log \frac{1}{\delta})^2})$ Bits Lower Bound}

In the previous section we have shown how to extract $\Theta(\log \frac{1}{\delta})$ words of information from a sampler. 
Our goal is to extract more words. New observation comes from the upper bound for constructing an $\ell_0$ sampler. 
When $\delta=\frac{1}{n}$, we have the following sampler algorithm that consumes $O(\log^3 n)$ bits. 
The sampler consists of $\log n$ layers, and on layer $i\in [\log n]$ it maintains a separate $\log n$-sparse recover system for the sub-stream generated by sub-sampling the items from the universe $[n]$ with probability $2^{-i}$.
Each sparse recovery system takes $O(\log^2 n)$ bits. 
Its correctness comes from the fact that 
\begin{enumerate}
  \item With probability at least $1-n^{-c}$ there is some layer $i$ that contains at least $1$ and at most $\log n$ items whose coordinates are non-zero. 
  \item Conditioned on the event that on layer $i$ the number of items whose coordinates are non-zero is between $1$ and $\log n$, the sparse recovery system on layer $i$ works with failure probability at most $n^{-c}$. In this context, we say the sparse recovery system works if it could recover at least one item. 
\end{enumerate}

Intuitively, the previous lower bound only extracts information from one single layer (i.e. layer $0$). In this section, we build a framework to extract information from multiple layers. A second technique that makes our improvement possible is bundling: after obtaining a sample from the sampler, we not only peel off the sample, but also remove the whole bundle containing the sample from the sampler, where bundles are a partition of elements inserted to the sampler.

\subsection{Protocol}

Alice wants to send random set $A$ to Bob where $|A|=m$. Similar to $\enc_1$, Alice constructs \samp by inserting all the items in $A$, and sends it to Bob. Moreover, Alice will send Bob a subset $B\subseteq A$ computed as follows. Initially $B=A$. Alice proceeds in $R$ rounds. On round $r$ ($r=1,\ldots, R$) Alice considers interval $I_r$ (where $I_1=\{0,\ldots, n-1\}$), and divides $I_r$ into $K$ even parts: $I_{r,1}, \ldots, I_{r,K}$. She samples a uniform random $k_r$ in $1,\ldots, K$, and sets the next interval to be $I_{r+1}=I_{r,k_r}$. She removes all elements except $A\cap I_r$ from \samp, denoted by $\samp_r$, and she wants to obtain $n_r$ items in $A\cap I_r$ from $\samp_r$: each time she obtains a sample $s$, she finds the $k$ in which $s \in I_{r,k}$, and remove $A\cap I_{r,k}$ from $\samp_r$. If $s$ is an invalid sample or $k=k_r$, Alice's encoder fails; otherwise she remove $s$ from $B$, and $s$ is considered to be the save with the help of \samp.  The decoding process is symmetric. 

The parameters, encoder and decoder are given as follows. 


\begin{algorithm}[H] \label{algo:para}
  \caption{Variables Shared by Alice's $\enc_4$ and Bob's $\dec_4$.}
  \begin{algorithmic}[1] 
    \State $m\leftarrow n^{0.99}$
    \State $K\leftarrow \log n \cdot \log \frac{1}{\delta}$
    \State $R\leftarrow \frac{\log n}{50 \log K}$
    \State $i_1\leftarrow 0$
    \State $\Delta_1\leftarrow n$
    \For {$r=1,\ldots, R-1$}
      \State Let $k_r$ be a uniform sample from $\{1, \ldots, K\}$
      \State $i_{r+1}\leftarrow i_r+\frac{k_r-1}{K}\Delta_r$
      \State $\Delta_{r+1}\leftarrow \frac{\Delta_r}{K}$
    \EndFor
    \For {$r=1,\ldots, R$}
      \State $n_r\leftarrow \frac{1}{2} \cdot \frac{\log \frac{1}{\delta}}{\log K}$
      \State $I_r \leftarrow \{i\in \mathbb{N}|i_r\le i < i_r+\Delta_r\}$
      \For {$k=1,\ldots, K$}
        \State $I_{r,k}\leftarrow \{i\in \mathbb{N}|i_r+(k-1)\frac{\Delta_r}{K}\le i < i_r+k\frac{\Delta_r}{K}\}$
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Alice's Encoder.}
  \begin{algorithmic}[1]
    \Procedure{$\enc_4$}{$A$}
    \State $\samp \leftarrow \emptyset$
    \State Insert all elements in $A$ into \samp
    \State $B\leftarrow A$
    \For {$r=1,\ldots, R$}
      \State Let $\samp_r$ be a copy of \samp
      \State Remove all elements in $A\backslash (A\cap I_r)$ from $\samp_r$
      \For {$i=1,\ldots, n_r$}
        \State Obtain a sample $s$ from $\samp_r$ 
        \If {$s$ is not valid} 
          \State \Return ``\textbf{fail}'' \label{algo-enc_4:sampler-wrong}
        \EndIf
        \State Find $k$ such that $s\in I_{r,k}$ 
        \If {$k=k_r$} 
          \State \Return ``\textbf{fail}'' \label{algo-enc_4:kill-wrong}
        \EndIf
        \State Remove all elements in $A\cap I_{r,k}$ from $\samp_r$
        \State $B \leftarrow B \backslash \{s\}$
      \EndFor
    \EndFor
    \State \Return $(\samp, B)$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
  \caption{Bob's Decoder.}
  \begin{algorithmic}[1]
    \Procedure{$\dec_4$}{\samp, $B$}
    \State $A\leftarrow B$
    \For {$r=1,2,\ldots,R$}
      \State Let $\samp_r$ be a copy of \samp 
      \State Remove all the items in $A\backslash (A\cap I_r)$ from $\samp_r$
      \For {$i=1,\ldots, n_r$}
      \State Obtain a sample $s$ from $\samp_r$
      \State $A\leftarrow A \cup \{s\}$
      \State Find $k$ such that $k\in I_{r,k}$
      \State Remove items in $A\cap I_{r,k}$ from $\samp_r$
      \EndFor
    \EndFor
    \State \Return $A$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Analysis} 

Note that $\E(|A\cap I_{r,k}|)=m\cdot K^{-r}$. By the choice of parameters we have $m=n^{0.99}$, $K^{-r}\ge K^{-R}=n^{-0.02}$ (note that the range of $\delta$ is specified by Formula~\ref{formula:delta-range}). Therefore $\E(|A\cap I_{r,k}|) \ge n^{0.97}$, for $r=1,\ldots, R$ and $k=1,\ldots, K$. For any pair of $(r,k)$, because of the randomness in $A$, we can prove the probability that $A\cap I_{r,k}$ is empty is exponentially small. By union bound, with high probability, all $A\cap I_{r,k}$ are not empty. In the following, we will discuss conditioned on that.

\begin{lemma} \label{lemma:enc_4-sampler-work}
  With probability at least $\frac{9}{10}$, $\enc_4$ does not return ``fail''. 
\end{lemma}

\begin{proof}
  The probability that $ENC_4$ returns fail on line~\ref{algo-enc_4:sampler-wrong} is at most $\sum_{i=1}^{R} K^{n_i}\cdot \delta = \frac{\delta^{1/2} \log n}{50\log K} < \frac{1}{50}$. 
  
  The probability that $ENC_4$ returns fail on line~\ref{algo-enc_4:kill-wrong} is at most $\sum_{r=1}^{R}{\frac{n_r}{K}}=\frac{1}{100\log^2 K}<\frac{1}{100}$.
  
\end{proof}

\begin{lemma}
  If $\enc_4$ does not return ``fail'', we have $\dec_4(\enc_4(A))=A$.
\end{lemma}

\begin{lemma} \label{lemma:words-saving}
  If $\enc_4$ does not return ``fail'', we have $|A|-|B|=\Omega(\frac{\log n \log \frac{1}{\delta}}{(\log\log n + \log\log \frac{1}{\delta})^2})$, where $B$ is the set $\enc_4$ outputs.
\end{lemma}

\begin{proof}
  $|A|-|B|=\sum_{r=1}^{R}{n_r}= \frac{\log n}{50 \log K} \cdot \frac{1}{2}\cdot \frac{\log \frac{1}{\delta}}{\log K}
  = \Omega(\frac{\log n \log \frac{1}{\delta}}{(\log\log n + \log\log \frac{1}{\delta})^2})$.
\end{proof}


\begin{lemma} \label{lemma:bits-saving}
  Let $m=n^{0.99}$. Let $X\in \mathbb{N}$ be a random variable, and $X\le m$. Moreover, $\E(X)\le m-d$. We have $\E(\log {n \choose m}-\log {n \choose X})=\Omega(d \log n)$.
\end{lemma}

\begin{proof}
  \begin{align*}
    \log {n \choose m}-\log {n \choose X}
    &=   \log \frac{n!/(m!(n-m)!)}{n!/(X!(n-X)!)} \\
    &=   \sum_{i=1}^{m-X}\log \frac{n-X-i+1}{m-i+1} \\
    &\ge (m-X)\cdot \log \frac{n-X}{m} \\
    &\ge (m-X)\cdot \log n^{1/200}
  \end{align*}
  
  Taking expectation on both sides, we get $\E(\log {n \choose m}-\log {n \choose X})\ge \frac{d}{200} \log n$. 
\end{proof}

\begin{theorem}
  $\s = \Omega(\frac{\log^2 n\log{\frac{1}{\delta}}}{(\log\log n+\log\log \frac{1}{\delta})^2})$ for $2^{-n^{0.01}}<\delta<(\log n)^{-2}$.
\end{theorem}

\begin{proof}
  Let \success be the conjunction of the following events:
  \begin{enumerate}
    \item All $A \cap I_{r,k}$ are non-empty ($r=1,\ldots, R, k=1,\ldots, K$).
    \item $\enc_4$ does not return ``fail'' on input $A$.
  \end{enumerate}
  By Lemma~\ref{lemma:enc_4-sampler-work}, we have $\Pr(\success)\ge \frac{1}{2}$. By Lemma~\ref{lemma:lb-meta}, we have $\s\ge \log (^n_m) - s' -2$. By definition, $s'=\log n + \E(\log (^n_{|B|})|\success)$. By Lemma~\ref{lemma:words-saving} and Lemma~\ref{lemma:bits-saving}, we get $\E(\log (^n_{|B|})|\success)=\log (^n_m)-\Omega(\frac{\log^2 n\log{\frac{1}{\delta}}}{(\log\log n+\log\log \frac{1}{\delta})^2})$. 
\end{proof}

\section{$\Omega(\log^2 n \log \frac{1}{\delta})$ Bits Lower Bound}

Let $R=\frac{1}{10}\log n \log \frac{1}{\delta}$ and let $K=\log \frac{1}{\delta}$. 

\begin{algorithm}[H]
  \caption{Alice's Encoder.}
  \begin{algorithmic}[1]
    \Procedure{$\enc$}{$A$}
    \State $\samp \leftarrow \emptyset$
    \State Insert items in $A$ into \samp
    \State $A_1 \leftarrow A$
    \State $S\leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
      \State Let $\samp_r$ be a copy of $\samp$
      \State Remove items in $A\backslash A_r$ from $\samp_r$ \Comment Now $\samp_r$ contains the elements in $A_r$
      \State Let $s_r$ be a sample from $\samp_r$
      \State $S\leftarrow S \cup \{s_r\}$
      \State $A_{r+1}\leftarrow A_r \backslash \{s_r\}$
      \For {$a\in A_{r+1}$}
        \State With probability $\frac{1}{K}$, $A_{r+1}\leftarrow A_{r+1}\backslash \{a\}$ \Comment{Use the randomness that Alice and Bob share}
      \EndFor
    \EndFor
    \State \Return ($A\backslash S$, \samp) 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Bob's Decoder.}
  \begin{algorithmic}[1]
    \Procedure{$\dec$}{$B$, \samp}
    \State $S\leftarrow \emptyset$
    \State $C_1 \leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
    \State Let $\samp_r$ be a copy of $\samp$
    \State Remove items in $C_r$ from $\samp_r$
    \State Let $s_r$ be a sample from $\samp_r$
    \State $S\leftarrow S \cup \{s_r\}$
    \State $C_{r+1}\leftarrow C_r \cup \{s_r\}$
    \For {$a\in B\backslash C_{r+1}$}
    \State With probability $\frac{1}{K}$, $C_{r+1}\leftarrow C_{r+1}\cup \{a\}$ \Comment{Use the randomness that Alice and Bob share,}
    \EndFor \Comment{so that $C_r=A\backslash A_r$ ($A, A_r$ are defined in $\enc$)}
    \EndFor
    \State \Return $B\cup S$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Analysis}

\begin{lemma}
  Let function $f$: $\{0,1\}^n\times \{0,1\}^m\rightarrow \{0,1\}$. Let $X$ be a uniformly random string in $\{0,1\}^n$. If for any $y\in \{0,1\}^m$ we have $\Pr(f(X,y)=1)\le \delta$ where $0<\delta<1$, then for any random variable $Y$ in $\{0,1\}^m$, we have 
  $$\Pr(f(X,Y)=1)\le \frac{I(X;Y)+1}{\log \frac{1}{\delta}},$$ 
  where $I(X;Y)$ is the mutual information (in bits) between $X$ and $Y$.
\end{lemma}

\begin{proof}
  It is equivalent to prove $I(X;Y)\ge \E(f(X,Y))\cdot \log\frac{1}{\delta}-1$. By definition of mutual entropy, $I(X;Y)=H(X)-H(X|Y)$ where $H(X)=n$ and $H(X|Y)\le 1+(1-\E(f(X,Y)))\cdot n+\E(f(X,Y))\cdot (n-\log\frac{1}{\delta})=n+1-\E(f(X,Y))\cdot \log\frac{1}{\delta}$.
  The upper bound for $H(X|Y)$ is obtained by considering the following one-way communication problem: Alice obtains both $X$ and $Y$ while Bob only gets $Y$, what is the (minimum) expected number of bits that Alice sends to Bob so that Bob can recover $X$? Any protocol gives an upper bound for $H(X|Y)$, and we simply take the following protocol: first Alice sends Bob $f(X,Y)$ (taking $1$ bit); and then if $f(X,Y)=0$ Alice sends $X$ directly (taking $n$ bits), otherwise, $f(X,Y)=1$, Alice sends the index of $X$ in $\{x|f(x,Y)=1\}$ (taking $\log (\delta 2^n)=n-\log\frac{1}{\delta}$ bits).  
\end{proof}

We do the analysis conditioned on $A$. Let $X$ denote the random source used by the sampler. The probability that the sampler fails is upper bounded by $\sum_{r=1}^{R}{2^{I(X;A_r)}\cdot \delta}$. 

\bibliographystyle{alpha}
\bibliography{shortbib}

\end{document}
























