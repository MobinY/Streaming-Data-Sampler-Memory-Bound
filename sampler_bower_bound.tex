\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx} 
\usepackage{filecontents}
\usepackage[usenames]{color}
\usepackage{algorithm,algpseudocode,float}
\usepackage{xspace}

\newcommand{\EDIT}[1]{\textcolor{red}{\textit{#1}}}

\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\DeclareMathOperator*{\eps}{\varepsilon}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}

\usepackage{enumerate}

\newcommand{\samp}{\textsf{SAMP}\xspace}
\newcommand{\success}{\textsf{SUCC}\xspace}
\newcommand{\enc}{\textsf{ENC}\xspace}
\newcommand{\dec}{\textsf{DEC}\xspace}
\newcommand{\s}{\textsf{s}\xspace}

\title{A Note on Space Lower Bound for Samplers}


\begin{document}
	
\maketitle

We study the space lower bound for maintaining a sampler over a turnstile stream. An $\ell_p$-sampler with failure probability at most $\delta$ is a randomized data structure for maintaining vector $x\in \mathbb{R}^n$ (initially 0) under a stream of updates in the form of $(i, \Delta)$ (meaning that $x_i \leftarrow x_i+\Delta$); in the end, with probability at least $1-\delta$, it gives an ``$\ell_p$-sample'' according to $x$: namely, item $i$ is sampled with probability $\frac{|x_i|^p}{\sum_{j\in [n]}{|x_j|^p}}$. 

Note that updates are independent of the randomness used in the sampler. That is, for the purpose of proving a lower bound, we assume an oblivious adversary. 

To the best of my knowledge, the best space upper bound for $\ell_0$ sampler is $O(\log^2 n \log \frac{1}{\delta})$ bits, while the previous best lower bound is $\Omega(\log^2 n +\log\frac{1}{\delta})$ bits (where $\Omega(\log^2 n)$ is shown in \cite{jowhari2011tight}). The bound is tight for constant $\delta$, while for example, when $\delta=\frac{1}{n}$, the gap is $\log n$. 

We assume that 
\begin{align} \label{formula:delta-range}
2^{-n^{c_1}}<\delta<(\log n)^{-c_2},
\end{align}
where $c_1=0.9$ and $c_2=1.1$. For other range of $\delta$ we will study later.

In this note, we show space lower bounds for maintaining a sampler for a {\em binary} vector. That is, at any time, we are guaranteed that $x\in \{0,1\}^n$. This makes our result strong in the sense that (1) the lower bound applies for any $p$; (2) the lower bound also works for strict turnstile streams.

In the following sections, we give sequentially improved lower bounds. First, we give a lower bound of $\Omega(\log n \log \frac{1}{\delta})$ bits. Then, we improve it to $\Omega(\log n \log {\frac{1}{\delta}} \log \frac{\log n}{\log\log \frac{1}{\delta}})$ bits, and finally $\Omega(\frac{\log^2 n \log \frac{1}{\delta}}{\log\log n + \log\log \frac{1}{\delta}})$ bits. The lower bounds are based on communication complexity in the public random coin model. Alice wants to send Bob a uniform random set $A\subseteq [n]$ of size $m$ (Bob knows $m$, but the random source generating $A$ is independent of the random source accessible to Bob). The one-way communication problem is: Alice sends some message to Bob, and Bob is required to recover $A$ completely. Since the randomness in $A$ contains $\log (^n_m)$ bits of information, any randomized protocol requires at least $\log (^n_m)$ expected bits. 

Now Alice considers to attach (the memory of) a sampler \samp in the message. The sampler uses public random coins as its random source, so that the sampler will behave the same at Alice's and Bob's as long as the updates are all the same. Alice will insert all the items in $A$ into \samp and send \samp to Bob. In addition, Alice will send a subset $B\subseteq A$ to Bob, so that together with $B$ and \samp, Bob is able to recover $A$ with good probability based on some protocol they have agreed on. 
 
Now we turn the previous protocol into a new one without any failure. Let \success denote the event (or a subset of the event) that Bob successfully recovers $A$ (note that Alice can simulate Bob, so she knows exactly when \success happens). If \success happens Alice will send Bob a message starting with a $1$, followed by (the memory of) \samp, then followed by the native encoding of $B$; otherwise, Alice will send a message starting with a $0$, followed by the native encoding of $A$. We say the native encoding of a set $S\subseteq [n]$ to be an integer (expressed in binary) in $[{n \choose |S|}]$ together with $|S|$ (taking $\log n$ bits). We drop the size of the set if it is known by the receiver.

\begin{lemma} \label{lemma:lb-meta}
  Let $\s$ denote the space (in bits) used by a sampler with failure probability at most $\delta$. Let $\s'$ denote the expected number of bits to represent $B$ conditioned on \success (if we need to send some extra auxiliary information, we will also count it into $\s'$). We have 
  
  $$(1+\s+\s')\cdot \Pr(\success)+(1+\log (^n_m)) \cdot (1-\Pr(\success)) \ge \log (^n_m).$$
  
  If $\Pr(\success)\ge 1/2$, we have 
  
  \begin{align} \label{formula:lb-meta}
  \s\ge \log (^n_m) - \s' - 2.
  \end{align} 
\end{lemma}

%=====================================================================
\section{$\Omega(\log n \log {\frac{1}{\delta}})$ Bits Lower Bound}
Let $m=\frac{1}{2}\log \frac{1}{\delta}$, namely, Alice wants to send a uniform random set $A\subseteq [n]$ of size $\frac{1}{2}\log\frac{1}{\delta}$ to Bob. Let $A=\{a_1,\ldots,a_m\}$ and $a_1<\ldots<a_m$. 

\begin{algorithm}[H]
\caption{Alice's Encoder.}
\begin{algorithmic}[1]
\Procedure{$\enc_1$}{$A$}
  \State $\samp \leftarrow \emptyset$
  \For {$i=1,2,\ldots,m$}
    \State Insert $a_i$ into \samp
  \EndFor
  \State \Return \samp 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Bob's Decoder.}
\begin{algorithmic}[1]
\Procedure{$\dec_1$}{\samp}
  \State $S\leftarrow \emptyset$
  \For {$i=1,2,\ldots,m$}
    \State Let $\samp_i$ be a copy of \samp \Comment{So that $\samp_i$ can behave as if it is \samp}
    \For {$s \in S$} \Comment{Enumerate the elements in $S$ from smallest to biggest}
      \State Remove $s$ from $\samp_i$
    \EndFor
    \State Obtain a sample $s_i$ from $\samp_i$
    \State $S \leftarrow S \cup \{s_i\}$
  \EndFor
  \State \Return $S$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
  For any $A\subseteq [n]$, where $|A|=m=\frac{1}{2}\log \frac{1}{\delta}$, $\Pr(\dec_1(\enc_1(A))=A)\ge 1/2$.
\end{lemma}

\begin{proof}
  Let $E_S$ denote the event that after removing all the items in $S$ (in the order from smallest to biggest) from \samp, it gives a valid sample when queried. We have 
  
  $$\Pr(\dec_1(\enc_1(A))=A)
  \ge \Pr(\bigcap_{S\subseteq A, S\ne \emptyset}{E_S})
  \ge 1 - \sum_{S\subseteq A, S\ne \emptyset}{\Pr({\overline {E_S}})} 
  \ge 1 - \delta \cdot 2^{\frac{1}{2}\log \frac{1}{\delta}}
  \ge 1/2.
  $$  
\end{proof}

\begin{lemma}
  $\s = \Omega(\log n \log \frac{1}{\delta}).$
\end{lemma}

\begin{proof}
  It follows from Formula~\ref{formula:lb-meta}, where $\log {n \choose \frac{1}{2}\log \frac{1}{\delta}}=\Omega(\log n \log \frac{1}{\delta})$ and $\s'=0$. 
\end{proof}

\begin{remark}
  The following decoder $\dec_1'$ is similar to $\dec_1$, but we will lose a factor of $\log\log \frac{1}{\delta}$ in the lower bound because by doing so we have to union-bound $O(m!)$ events instead of $O(2^m)$ events (so that in turn we have to set $m$ to be $\frac{\log \frac{1}{\delta}}{\log\log\frac{1}{\delta}}$ in order to have good success probability). 
\end{remark}

\begin{algorithm}[H]
\caption{A Worse Decoder.}
\begin{algorithmic}[1]
\Procedure{$\dec_1'$}{\samp}
\State $S\leftarrow \emptyset$
\For {$i=1,2,\ldots,m$}
\State Obtain a sample $s_i$ from $\samp$
\State Remove $s_i$ from $\samp$
\State $S \leftarrow S \cup \{s_i\}$
\EndFor
\State \Return $S$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}


%=====================================================================
\section{$\Omega(\log n \log {\frac{1}{\delta}} \log \frac{\log n}{\log\log \frac{1}{\delta}})$ Bits Lower Bound}

In the previous section we have shown how to extract $\Theta(\log \frac{1}{\delta})$ words of information from a sampler. 
Our goal is to extract more words. New observation comes from the upper bound for constructing an $\ell_0$ sampler. 
Let $\delta=\frac{1}{n}$, we have the following sampler algorithm that consumes $O(\log^3 n)$ bits. 
The sampler consists of $\log n$ layers, and on layer $i\in [\log n]$ it maintains a separate $\log n$-sparse recover system for the sub-stream generated by sub-sampling the items from the universe $[n]$ with probability $2^{-i}$.
Each sparse recovery system takes $O(\log^2 n)$ bits. 
Its correctness comes from the fact that 
\begin{enumerate}
\item With probability at least $1-n^{-c}$ there is some layer $i$ that contains at least $1$ and at most $\log n$ items whose coordinates are non-zero. 
\item Conditioned on the event that on layer $i$ the number of items whose coordinates are non-zero is between $1$ and $\log n$, the sparse recovery system on layer $i$ works with failure probability at most $n^{-c}$. In this context, we say the sparse recovery system works if it could recover at least one item. 
\end{enumerate}

Intuitively, the previous lower bound only extracts information from one single layer (i.e. layer $0$). In this section, we build a framework to extract information from $\Theta(\log n)$ layers. 

Alice wants to send random set $A$ to Bob where $|A|=m=n^{1/4}$. Similar to $\enc_1$, Alice constructs \samp by inserting all the items in $A$, and sends it to Bob. Moreover, Alice will send Bob a subset $B\subseteq A$ computed as follows. Initially $B=A$. Alice proceeds in $R=\log m - \log\log \frac{1}{\delta}$ rounds. Let $A_r = \{a\in A|a<\frac{n}{2^{r-1}}\}$. Furthermore, let $A_r^{-}=A_{r+1}$ and $A_r^{+}=A_r \backslash A_r^{-}$. On round $r$ (for $r=1,\ldots,R$):

\begin{enumerate}
\item Alice makes a copy of \samp, denoted by $\samp_r$, and removes all items in $A\backslash A_r$ from $\samp_r$.  
\item Similar to $\enc_1'$, obtain $n_r=\frac{1}{2}\cdot \frac{\log \frac{1}{\delta}}{(\log m)+2-r}$ samples from $\samp_r$, denoted by $S_r$. 
\item Remove items in $S_r\cap A_r^{+}$ from $B$.
\end{enumerate}

\begin{algorithm}[H]
  \caption{Alice's Encoder.}
  \begin{algorithmic}[1]
    \Procedure{$\enc_2$}{$A$}
    \State $\samp \leftarrow \emptyset$
    \State Insert all elements in $A$ into \samp
    \State $B\leftarrow A$
    \For {$r=1,\ldots, R$}
      \State Let $\samp_r$ be a copy of \samp
      \State Remove all elements in $A\backslash A_r$ from $\samp_r$
      \State $S_r\leftarrow \emptyset$
      \For {$i=1,\ldots, n_r$}
        \State Obtain a sample $s$ from $\samp_r$
        \State Remove $s$ from $\samp_r$
        \State $S_r\leftarrow S_r \cup \{s\}$
      \EndFor
      \State $B \leftarrow B \backslash (A_r^{+}\cap S_r)$
    \EndFor
    \State \Return $(\samp, B)$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Bob's Decoder.}
  \begin{algorithmic}[1]
    \Procedure{$\dec_2$}{\samp, $B$}
    \State $A\leftarrow B$
    \For {$r=1,2,\ldots,R$}
      \State Let $\samp_r$ be a copy of \samp 
      \State Remove all the items in $\{a\in A|a\ge \frac{n}{2^{r-1}}\}$ from $\samp_r$
      \State $S_r\leftarrow \emptyset$
      \For {$i=1,\ldots, n_r$}
      \State Obtain a sample $s$ from $\samp_r$
      \State Remove $s$ from $\samp_r$
      \State $S_r\leftarrow S_r \cup \{s\}$
      \EndFor
      \State $A \leftarrow A \cup \{a\in S_r|a \ge \frac{n}{2^r}\}$
    \EndFor
    \State \Return $A$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


%=====================================================================
\section{Lower Bound for a Promised Problem}
Peeling off by chunks (instead of one by one). 

%=====================================================================
\section{$\Omega(\frac{\log^2 n\log{\frac{1}{\delta}}}{\log\log n+\log\log \frac{1}{\delta}})$ Bits Lower Bound}

Let $m=n^{0.99}$, $K=\log n \cdot \log \frac{1}{\delta}$ and $R=\frac{1}{50}\log n$. Let $A_r = \{a\in A|a<\frac{n}{2^{r-1}}\}$. Let $A_r^{-}=A_{r+1}$ and $A_r^{+}=A_r \backslash A_r^{-}$. We partition each $A_r$ into $K$ chunks, where chunk $k$ ($k=1,\ldots,K$) $C_{r,k}=\{a\in A|\frac{k-1}{K} \le \frac{a}{n/2^{r-1}} < \frac{k}{K} \}$. Assume that $K$ is dividable by $2$ so that $A_r^{-}$ contains the first $\frac{K}{2}$ chunks and $A_r^{+}$ contains the last $\frac{K}{2}$ chunks. Let $n_r=\frac{1}{20} \cdot \frac{\log \frac{1}{\delta}}{\log K}$.
All these parameters are shared by Alice's $\enc_4$ and Bob's $\dec_4$. 

\begin{algorithm}[H]
  \caption{Alice's Encoder.}
  \begin{algorithmic}[1]
    \Procedure{$\enc_4$}{$A$}
    \State $\samp \leftarrow \emptyset$
    \State Insert all elements in $A$ into \samp
    \State $B\leftarrow A$
    \State $C\leftarrow \emptyset$
    \For {$r=1,\ldots, R$}
      \State Let $\samp_r$ be a copy of \samp
      \State Remove all elements in $A\backslash A_r$ from $\samp_r$
      \State $S_r\leftarrow \emptyset$
      \For {$i=1,\ldots, n_r$}
        \State Obtain a sample $s$ from $\samp_r$
        \State Find $k$ such that $s\in C_{r,k}$
        \State Remove all elements in $C_{r,k}$ from $\samp_r$
        \State $S_r\leftarrow S_r \cup \{s\}$
        \If {$k\le \frac{K}{2}$}
          \State $C\leftarrow C \cup C_{r,k}$
        \EndIf
      \EndFor
      \State $B \leftarrow B \backslash (A_r^{+}\cap S_r)$
    \EndFor
    \State $B\leftarrow B \cup C$
    \State \Return $(\samp, B)$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
  \caption{Bob's Decoder.}
  \begin{algorithmic}[1]
    \Procedure{$\dec_4$}{\samp, $B$}
    \State $A\leftarrow B$
    \For {$r=1,2,\ldots,R$}
      \State Let $\samp_r$ be a copy of \samp 
      \State Remove all the items in $\{a\in A|a\ge \frac{n}{2^{r-1}}\}$ from $\samp_r$
      \For {$i=1,\ldots, n_r$}
      \State Obtain a sample $s$ from $\samp_r$
      \State $A\leftarrow A \cup \{s\}$
      \State Find $k$ such that $\frac{k-1}{K} \le \frac{s}{n/2^{r-1}} < \frac{k}{K}$
      \State Remove items in $\{a\in A|\frac{k-1}{K} \le \frac{a}{n/2^{r-1}} < \frac{k}{K} \}$ from $\samp_r$
      \EndFor
    \EndFor
    \State \Return $A$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Analysis} 

Note that $\E(|C_{r,k}|)=m\cdot 2^{1-r}\cdot \frac{1}{K}$. By the setting of parameters we have $m=n^{0.99}$, $2^{1-r}\ge 2^{-R}=n^{-0.02}$ and $\frac{1}{K}=\frac{1}{\log n \log \frac{1}{\delta}}\ge n^{-0.91}$ (note that the range of $\delta$ is specified by Formula~\ref{formula:delta-range}). Therefore $\E(|C_{r,k}|) \ge n^{0.07}$, for $r=1,\ldots, R$ and $k=1,\ldots, K$. Let $\eps=0.01$. For any pair of $(r,k)$, because of the randomness in $A$, we can prove by Chernoff bound (and negative correlation) that the probability that $||C_{r,k}|-\E(|C_{r,k}|)|>\eps \E(|C_{r,k}|)$ is exponentially small. By union bound, with high probability, for all $C_{r,k}$, its size deviates its expectation by a factor of at most $\eps$. In the following, we will discuss conditioned on that.

\begin{lemma} \label{lemma:enc_4-sampler-work}
  With probability at least $\frac{9}{10}$, all the queries to the samplers in $\enc_4$ are answered successfully. 
\end{lemma}

\begin{proof}
  By union bound, the failure probability is at most $\sum_{i=1}^{R} K^{n_i}\cdot \delta = \frac{\delta^{19/20} \log n}{50} < \frac{1}{10}$. 
\end{proof}

\begin{lemma}
  Conditioned on the samplers answer all the queries successfully, we have $\dec_4(\enc_4(A))=A$.
\end{lemma}

\begin{lemma} \label{lemma:words-saving}
  Conditioned on the samplers answer all the queries successfully, we have $\E(|A|-|B|)=\Omega(\frac{\log n \log \frac{1}{\delta}}{\log\log n + \log\log \frac{1}{\delta}})$, where $B$ is the set $\enc_4$ outputs.
\end{lemma}

\begin{proof}
  Let $Y_{r,k}$ ($r=1,\ldots, R$ and $k=\frac{K}{2}+1,\ldots, K$) denote the indicator random variable that on round $r$ the sampler returns a sample $s$ so that $s\in C_{r,k}$ and on that round $s\not\in C$ (or equivalently, $s\in A\backslash B$).  
  
  The probability that on round $r$ the sampler return a sample $s$ so that $s\in C_{r,k}$ is at least $\frac{1-\eps}{1+\eps}\cdot \frac{n_r}{K}$. The probability that $s\in C$ is at most $\sum_{i=1}^{r-1}{\frac{1+\eps}{1-\eps}\cdot \frac{n_i}{K}}$. This two events are independent, so 
  
  $$\E(Y_{r,k})\ge \frac{1-\eps}{1+\eps}\cdot \frac{n_r}{K} \cdot (1-\sum_{i=1}^{r-1}{\frac{1+\eps}{1-\eps}\cdot \frac{n_i}{K}}) \ge \frac{n_r}{2K}=\frac{\log \frac{1}{\delta}}{40K\log K}=\frac{\log \frac{1}{\delta}}{40K(\log\log n + \log\log \frac{1}{\delta})}.$$
  
   We have $|A|-|B|=\sum_{r=1}^{R}{\sum_{k=\frac{K}{2}+1}^{K}{Y_{r,k}}}$. Taking the expectation on both sides we get the desired bound.
  
\end{proof}

\begin{lemma} \label{lemma:bits-saving}
  Let $m=n^{0.99}$. Let $X\in \mathbb{N}$ be a random variable, and $X\le m$. Moreover, $\E(X)\le m-d$. We have $\E(\log {n \choose m}-\log {n \choose X})=\Omega(d \log n)$.
\end{lemma}

\begin{proof}
  \begin{align*}
    \log {n \choose m}-\log {n \choose X}
    &=   \log \frac{n!/(m!(n-m)!)}{n!/(X!(n-X)!)} \\
    &=   \sum_{i=1}^{m-X}\log \frac{n-X-i+1}{m-i+1} \\
    &\ge (m-X)\cdot \log \frac{n-X}{m} \\
    &\ge (m-X)\cdot \log n^{1/200}
  \end{align*}
  
  Taking expectation on both sides, we get $\E(\log {n \choose m}-\log {n \choose X})\ge \frac{d}{200} \log n$. 
\end{proof}

\begin{theorem}
  $\s = \Omega(\frac{\log^2 n\log{\frac{1}{\delta}}}{\log\log n+\log\log \frac{1}{\delta}})$ for $2^{-n^{0.9}}<\delta<(\log n)^{-1.1}$.
\end{theorem}

\begin{proof}
  Let \success be the conjunction of the following events:
  \begin{enumerate}
    \item For all $C_{r,k}$ ($r=1,\ldots, R, k=1,\ldots, K$), $||C_{r,k}|-\E(|C_{r,k}|)|\le \eps \E(|C_{r,k}|)$.
    \item The samplers in $\enc_4$ answer all the queries successfully on input $A$.
  \end{enumerate}
  By Lemma~\ref{lemma:enc_4-sampler-work}, we have $\Pr(\success)\ge \frac{1}{2}$. By Lemma~\ref{lemma:lb-meta}, we have $\s\ge \log (^n_m) - s' -2$. By definition, $s'=\log n + \E(\log (^n_{|B|})|\success)$. By Lemma~\ref{lemma:words-saving} and Lemma~\ref{lemma:bits-saving}, we get $\E(\log (^n_{|B|})|\success)=\log (^n_m)-\Omega(\frac{\log^2 n\log{\frac{1}{\delta}}}{\log\log n+\log\log \frac{1}{\delta}}))$. 
\end{proof}

\bibliographystyle{alpha}
\bibliography{shortbib}

\end{document}
























