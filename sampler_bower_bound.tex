\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{algorithm,algpseudocode,float}
\usepackage{xspace}
%\usepackage{hyperref}
%\usepackage[usenames]{color}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\newcommand{\samp}{\textsf{SAMP}\xspace}
\newcommand{\success}{\textsf{SUCC}\xspace}
\newcommand{\enc}{\textsf{ENC}\xspace}
\newcommand{\dec}{\textsf{DEC}\xspace}
\newcommand{\s}{\textsf{s}\xspace}

\title{A Note on Space Lower Bound for Samplers}
\author{Jelani Nelson, Jakub Pachocki, Zhengyu Wang}

\begin{document}
	
\maketitle

We study the space lower bound for maintaining a sampler over a turnstile stream. An $\ell_p$-sampler with failure probability at most $\delta$ is a randomized data structure for maintaining vector $x\in \mathbb{R}^n$ (initially 0) under a stream of updates in the form of $(i, \Delta)$ (meaning that $x_i \leftarrow x_i+\Delta$); in the end, with probability at least $1-\delta$, it gives an ``$\ell_p$-sample'' according to $x$: namely, item $i$ is sampled with probability $\frac{|x_i|^p}{\sum_{j\in [n]}{|x_j|^p}}$. 

Note that updates are independent of the randomness used in the sampler. That is, for the purpose of proving a lower bound, we assume an oblivious adversary. 

To the best of our knowledge, the best space upper bound for $\ell_0$ sampler is $O(\log^2 n \log \frac{1}{\delta})$ bits, while the previous best lower bound is $\Omega(\log^2 n +\log\frac{1}{\delta})$ bits (where $\Omega(\log^2 n)$ is shown in \cite{jowhari2011tight}). The bound is tight for constant $\delta$, while for example, when $\delta=\frac{1}{n}$, the gap is $\log n$. 

We show space lower bounds for maintaining a sampler for a {\em binary} vector. That is, at any time, we are guaranteed that $x\in \{0,1\}^n$. This makes our result strong in the sense that (1) the lower bound applies for any $p$; (2) the lower bound also works for strict turnstile streams.

The lower bounds are based on communication complexity in the public random coin model. Alice wants to send Bob a uniform random set $A\subseteq [n]$ of size $m$ (Bob knows $m$, but the random source generating $A$ is independent of the random source accessible to Bob). The one-way communication problem is: Alice sends some message to Bob, and Bob is required to recover $A$ completely. Since the randomness in $A$ contains $\log (^n_m)$ bits of information, any randomized protocol that works with probability $1$ requires at least $\log (^n_m)$ expected bits. 

We consider the following protocol. Alice attaches (the memory of) a sampler \samp in the message. The sampler uses public random coins as its random source, so that the sampler will behave the same at Alice's and Bob's as long as the updates are all the same. Alice will insert all the items in $A$ into \samp and send \samp to Bob. In addition, Alice will send a subset $B\subseteq A$ to Bob, so that together with $B$ and \samp, Bob is able to recover $A$ with good probability based on some protocol they have agreed on. 
 
Now we turn the previous protocol into a new one without any failure. Let \success denote the event (or a subset of the event) that Bob successfully recovers $A$ (note that Alice can simulate Bob, so she knows exactly when \success happens). If \success happens Alice will send Bob a message starting with a $1$, followed by (the memory of) \samp, then followed by the native encoding (explained later) of $B$; otherwise, Alice will send a message starting with a $0$, followed by the native encoding of $A$. We say the native encoding of a set $S\subseteq [n]$ to be an integer (expressed in binary) in $[{n \choose |S|}]$ together with $|S|$ (taking $\log n$ bits). We drop the size of the set if it is known by the receiver.

\begin{lemma} \label{lemma:lb-meta}
  Let $\s$ denote the space (in bits) used by a sampler with failure probability at most $\delta$. Let $\s'$ denote the expected number of bits to represent $B$ conditioned on \success (if we need to send some extra auxiliary information, we will also count it into $\s'$). We have 
  
  \begin{align}
  (1+\s+\s')\cdot \Pr(\success)+(1+\log (^n_m)) \cdot (1-\Pr(\success)) \ge \log (^n_m).
  \end{align} 
  
  If $\Pr(\success)\ge 1/2$, we have 
  
  \begin{align} \label{formula:lb-meta}
  \s\ge \log (^n_m) - \s' - 2.
  \end{align} 
\end{lemma}

We consider the range of $\delta$ to be 
\begin{align} \label{formula:delta-range}
2^{-n^{c_1}}<\delta<c_2,
\end{align}
where $c_1=0.9$ and $c_2=2^{-10}$. In fact, our lower bound applies for the range of $\delta$ where $c_1$ and $c_2$ are any constants smaller than $1$. 

In Section~\ref{sec:simple-lb} we give a lower bound of $\Omega(\log n \log \frac{1}{\delta})$ bits. This illustrates some key ideas of our framework. Then we show a lower bound of $\Omega(\log^2 n \log \frac{1}{\delta})$ bits in Section~\ref{sec:optimal-lb}.

\begin{remark}
  Because the space lower bound in this note is proven via communication complexity under public random coin model, it also applies to non-uniform models of computation such as circuits and branching programs.  
\end{remark}

\begin{remark}
  The space lower bound in this note still applies if the sampler is required to output an arbitrary item whose coordinate is non-zero instead of a uniformly random one. 
\end{remark}

%=====================================================================
\section{$\Omega(\log n \log {\frac{1}{\delta}})$ Bits Lower Bound}\label{sec:simple-lb}
Let $m=\frac{1}{2}\log \frac{1}{\delta}$, namely, Alice wants to send a uniform random set $A\subseteq [n]$ of size $\frac{1}{2}\log\frac{1}{\delta}$ to Bob. Let $A=\{a_1,\ldots,a_m\}$ and $a_1<\ldots<a_m$. 

\begin{algorithm}[H]
\caption{Alice's Encoder.}
\begin{algorithmic}[1]
\Procedure{$\enc_1$}{$A$}
  \State $\samp \leftarrow \emptyset$ \Comment{\samp uses the random source shared by Alice and Bob}
  \For {$i=1,2,\ldots,m$}
    \State Insert $a_i$ into \samp
  \EndFor
  \State \Return \samp 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Bob's Decoder.}
\begin{algorithmic}[1]
\Procedure{$\dec_1$}{\samp}
  \State $S\leftarrow \emptyset$
  \For {$i=1,2,\ldots,m$}
    \State Let $\samp_i$ be a copy of \samp \Comment{So that $\samp_i$ can behave as if it is \samp}
    \For {$s \in S$} \Comment{Enumerate the elements in $S$ from smallest to biggest}
      \State Remove $s$ from $\samp_i$
    \EndFor
    \State Obtain a sample $s_i$ from $\samp_i$
    \State $S \leftarrow S \cup \{s_i\}$
  \EndFor
  \State \Return $S$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{lemma}
  For any $A\subseteq [n]$, where $|A|=m=\frac{1}{2}\log \frac{1}{\delta}$, $\Pr(\dec_1(\enc_1(A))=A)\ge 1/2$.
\end{lemma}

\begin{proof}
  Let $E_S$ denote the event that after removing all the items in $S$ (in the order from smallest to biggest) from \samp, it gives a valid sample when queried. We have 
  
  \begin{align}
  \Pr(\dec_1(\enc_1(A))=A)
  \ge \Pr(\bigcap_{S\subseteq A, S\ne \emptyset}{E_S})
  \ge 1 - \sum_{S\subseteq A, S\ne \emptyset}{\Pr({\overline {E_S}})} 
  \ge 1 - \delta \cdot 2^{\frac{1}{2}\log \frac{1}{\delta}}
  \ge 1/2.
  \end{align}
\end{proof}

\begin{lemma}
  $\s = \Omega(\log n \log \frac{1}{\delta})$.
\end{lemma}

\begin{proof}
  It follows from Formula~\ref{formula:lb-meta} in Lemma~\ref{lemma:lb-meta}, where $\log {n \choose \frac{1}{2}\log \frac{1}{\delta}}=\Omega(\log n \log \frac{1}{\delta})$ and $\s'=0$. 
\end{proof}

\begin{remark}
  The following decoder $\dec_1'$ is similar to $\dec_1$, but we will lose a factor of $\log\log \frac{1}{\delta}$ in the lower bound because by doing so we have to union-bound $O(m!)$ events instead of $O(2^m)$ events (so that in turn we have to set $m$ to be $\frac{\log \frac{1}{\delta}}{\log\log\frac{1}{\delta}}$ in order to have good success probability). 
\end{remark}

\begin{algorithm}[H]
\caption{A Worse Decoder.}
\begin{algorithmic}[1]
\Procedure{$\dec_1'$}{\samp}
\State $S\leftarrow \emptyset$
\For {$i=1,2,\ldots,m$}
\State Obtain a sample $s_i$ from $\samp$
\State Remove $s_i$ from $\samp$
\State $S \leftarrow S \cup \{s_i\}$
\EndFor
\State \Return $S$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}


%=====================================================================

\section{$\Omega(\log^2 n \log \frac{1}{\delta})$ Bits Lower Bound} \label{sec:optimal-lb}

In the previous section we have shown how to extract $\Theta(\log \frac{1}{\delta})$ words of information from a sampler. 
Our goal is to extract more words. New observation comes from the upper bound for constructing an $\ell_0$ sampler. 
When $\delta=\frac{1}{n}$, we have the following sampler algorithm that consumes $O(\log^3 n)$ bits. 
The sampler consists of $\log n$ layers, and on layer $i=1,\ldots, \log n$ it maintains a separate $\log n$-sparse recover system for the sub-stream generated by sub-sampling the items from the universe $[n]$ with probability $2^{-i}$.
Each sparse recovery system takes $O(\log^2 n)$ bits. 
Its correctness comes from the fact that 
\begin{enumerate}
  \item With probability at least $1-n^{-c}$ there is some layer $i$ that contains at least $1$ and at most $\log n$ items whose coordinates are non-zero. 
  \item Conditioned on the event that on layer $i$ the number of items whose coordinates are non-zero is between $1$ and $\log n$, the sparse recovery system on layer $i$ works with failure probability at most $n^{-c}$. In this context, we say the sparse recovery system works if it could recover at least one item. 
\end{enumerate}

Intuitively, the previous lower bound only extracts information from one single layer (i.e. layer $0$). In this section, we build a framework to extract information from multiple layers. A second technique that makes our improvement possible is random removal: after obtaining a sample from the sampler, we not only peel off the sample, but also randomly remove a fraction of items remaining in the sampler. 

\subsection{Protocol}

The parameters used by Alice and Bob are given in Algorithm~\ref{algo:para}.
Alice wants to send a uniformly random set $A$ to Bob where $|A|=m$. Similar to $\enc_1$, Alice constructs \samp by inserting all the items in $A$, and sends it to Bob. Note that \samp uses the same random source that Alice and Bob share. Moreover, Alice will send Bob a subset $B\subseteq A$ computed as follows. Initially $B=A$ and let $A_0=A$. Alice proceeds in $R$ rounds. On round $r$ ($r=1,\ldots, R$) Alice tries to obtain sample $s_r\in A_{r-1}$ from the sampler 
(In more detail, she will make a copy of \samp, denoted by $\samp_r$, remove all the items in $A\backslash A_{r-1}$ and query for a sample). 
Let $b$ denote a binary string of length $R$, where $b_r$ records whether the sampler succeeds on round $r$. In the end, Alice will also send $b$ to Bob. If $s_r\in A_{r-1}$, i.e., the sampler returns a valid sample, Alice will set $b_r=1$, and remove $s_r$ from $B$ (since $s_r$ can be obtained from the sampler, Alice does not need to include it in $B$. In Algorithm~\ref{algo:enc} Alice uses $S$ to keep track of $A\backslash B$); otherwise Alice will set $b_r=0$.
At the end of round $r$, Alice will generate a uniformly random set $A_r$, so that $A_r$ is a subset of $A_{r-1}\backslash \{s_r\}$ and $|A_r|=n_r$. 
In particular, Alice uses shared random source to generate $A_r$, so that Bob can recover $A_{r-1}\backslash A_r$ on round $r$. 
We present Alice's encoder in Algorithm~\ref{algo:enc}.

The decoding process is symmetric. Let $C_0=\emptyset$ and $S=\emptyset$. Bob proceeds in $R$ rounds. 
On round $r$ ($r=1,\ldots,R$), Bob obtains sample $s_r\in A\backslash C_{r-1}$ (In more detail, he will make a copy of \samp, denoted by $\samp_r$, remove all the items in $C_{r-1}$ and query for a sample). 
By the construction of $C_{r-1}$ that will be described later it is guaranteed that $A_{r-1}=A\backslash C_{r-1}$. 
Therefore, Bob will get exactly the same $s_r$ as Alice. 
Bob assigns initial value $C_{r-1}$ to $C_r$.
If $b_r=1$, Bob will add $s_r$ to both $S$ and $C_r$.
At the end of round $r$, Bob inserts a bunch of items to $C_r$ so that $C_r=A\backslash A_r$. Bob can achieve this because of the shared randomness when constructing $A_r$.
In the end, Bob's decoder outputs $B\cup S$.
We present the decoder in Algorithm~\ref{algo:dec}.

\begin{algorithm}[H] 
  \caption{Variables Shared by Alice's $\enc$ and Bob's $\dec$.} \label{algo:para}
  \begin{algorithmic}[1] 
    \State $m\leftarrow n^{0.99}$
    \State $K\leftarrow \frac{1}{10}\log \frac{1}{\delta}$
    \State $R\leftarrow \frac{1}{200}\log n \log \frac{1}{\delta}$
    \For {$r = 0, \ldots, R$}
      \State $n_r\leftarrow m \cdot 2^{-\frac{r}{K}}$ \Comment{By the setting of parameters we have $n_r-n_{r+1}\ge 2$}
    \EndFor
    \For {$a\in [n]$}
      \State Let $U_a$ be uniformly and independent sample from $[0,1]$ \Comment{Used to generate $A_r$, $r=1,\ldots, R$}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Alice's Encoder.} \label{algo:enc}
  \begin{algorithmic}[1]
    \Procedure{$\enc$}{$A$}
    \State $\samp \leftarrow \emptyset$
    \State Insert items in $A$ into \samp
    \State $A_0 \leftarrow A$
    \State $S\leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
      \State Let $\samp_r$ be a copy of $\samp$
      \State Remove items in $A\backslash A_{r-1}$ from $\samp_r$ \Comment So that $\samp_r$ contains the elements in $A_{r-1}$
      \State Let $s_r$ be a sample from $\samp_r$
      \State $A_r\leftarrow A_{r-1}$
      \If {$s_r\in A_r$} \Comment{i.e. if $s_r$ is a valid sample}
        \State $b_r\leftarrow 1$ \Comment{$b$ is a binary string of length $R$, indicating if sampler succeeds on round $r$}
        \State $S\leftarrow S \cup \{s_r\}$
        \State $A_r\leftarrow A_r \backslash \{s_r\}$
      \Else 
        \State $b_r\leftarrow 0$
      \EndIf
      \State Remove $|A_r|-n_r$ elements from $A_r$ with smallest $U_a$'s among $a\in A_r$ \Comment{So that $|A_r|=n_r$}
    \EndFor
    \State \Return ($A\backslash S$, $b$, \samp) 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Bob's Decoder.} \label{algo:dec}
  \begin{algorithmic}[1]
    \Procedure{$\dec$}{$B$, $b$, \samp}
    \State $S\leftarrow \emptyset$
    \State $C_0 \leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
      \State $C_r\leftarrow C_{r-1}$
      \If{$b_r=1$}
        \State Let $\samp_r$ be a copy of $\samp$
        \State Remove items in $C_{r-1}$ from $\samp_r$ \Comment{Invariant: $C_r=A\backslash A_r$ ($A_r$ is defined in $\enc$)}
        \State Let $s_r$ be a sample from $\samp_r$
        \State $S\leftarrow S \cup \{s_r\}$
        \State $C_r\leftarrow C_r \cup \{s_r\}$
      \EndIf
       \State Insert $m-n_r-|C_r|$ items into $C_r$ with smallest $U_a$'s among $a\in B\backslash C_r$
    \EndFor
    \State \Return $B\cup S$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Analysis}

\begin{lemma}\label{lemma:mutual-entropy}
  Let function $f$: $\{0,1\}^n\times \{0,1\}^m\rightarrow \{0,1\}$. Let $X$ be a uniformly random string in $\{0,1\}^n$. If for any $y\in \{0,1\}^m$ we have $\Pr(f(X,y)=1)\le \delta$ where $0<\delta<1$, then for any random variable $Y$ in $\{0,1\}^m$, we have 
  \begin{align}
  \Pr(f(X,Y)=1)\le \frac{I(X;Y)+1}{\log \frac{1}{\delta}},
  \end{align}
  where $I(X;Y)$ is the mutual information (in bits) between $X$ and $Y$.
\end{lemma}

\begin{proof}
  It is equivalent to prove $I(X;Y)\ge \E(f(X,Y))\cdot \log\frac{1}{\delta}-1$. By definition of mutual entropy, $I(X;Y)=H(X)-H(X|Y)$ where $H(X)=n$ and $H(X|Y)\le 1+(1-\E(f(X,Y)))\cdot n+\E(f(X,Y))\cdot (n-\log\frac{1}{\delta})=n+1-\E(f(X,Y))\cdot \log\frac{1}{\delta}$.
  The upper bound for $H(X|Y)$ is obtained by considering the following one-way communication problem: Alice obtains both $X$ and $Y$ while Bob only gets $Y$, what is the (minimum) expected number of bits that Alice sends to Bob so that Bob can recover $X$? Any protocol gives an upper bound for $H(X|Y)$, and we simply take the following protocol: first Alice sends Bob $f(X,Y)$ (taking $1$ bit); and then if $f(X,Y)=0$ Alice sends $X$ directly (taking $n$ bits), otherwise, $f(X,Y)=1$, Alice sends the index of $X$ in $\{x|f(x,Y)=1\}$ (taking $\log (\delta 2^n)=n-\log\frac{1}{\delta}$ bits).  
\end{proof}

We do the analysis conditioned on $A$. Let $X$ denote the random source used by the sampler. By Lemma~\ref{lemma:mutual-entropy}, the probability that the sampler fails on round $r$ is upper bounded by $\frac{I(X;A_r)+1}{\log \frac{1}{\delta}}$. Namely $\E(b_r)\ge 1-\frac{I(X;A_r)+1}{\log \frac{1}{\delta}}$. 

$I(X;A_r)=H(A_r)-H(A_r|X)$. Since $|A_r|=n_r$ and $A_r\subseteq A$ where $|A|=m$, $H(A_r)\le \log {m \choose n_r}$. Now we want to lower bound $H(A_r|X)$. By definition of conditional entropy, $H(A_r|X)=\sum_x{p_x\cdot H(A_r|X=x)}$. We fix an arbitrary $x$. If we can prove that for any $T\subseteq A$ where $|T|=n_r$, $\Pr(A_r=T|X=x)\le p$, then by definition of entropy we have $H(A_r|X=x)\ge\log\frac{1}{p}$. In fact, for any fixed $T$, we have

\begin{align}
H(A_r=T|X=x)\le \prod_{i=1}^{r}{\frac{{n_{i-1}-n_r-1 \choose n_{i-1}-n_i-1}}{{n_{i-1}-1 \choose n_{i-1}-n_i-1}}},
\end{align}

because on round $i$ ($1\le i \le r$), Alice removes $n_{i-1}-n_i$ elements from $A_{i-1}$ to get $A_i$. Conditioned on the event that $A_{i-1}\supseteq T$, the probability that $A_i\supseteq T$ is at most ${{n_{i-1}-n_r-1 \choose n_{i-1}-n_i-1}}/{{n_{i-1}-1 \choose n_{i-1}-n_i-1}}$, where the equation achieves when $s_i\in A_{i-1}\backslash T$, and we take a uniformly random subset of $A_{i-1}\backslash \{s_i\}$ of size $n_{i-1}-n_i-1$, so that the subset does not intersect with $T$.

For notation simplicity, let $n^{\underline{k}}$ denote $n\cdot (n-1)  \ldots (n-k+1)$. We have 

\begin{align}
\prod_{i=1}^{r}{\frac{{n_{i-1}-n_r-1 \choose n_{i-1}-n_i-1}}{{n_{i-1}-1 \choose n_{i-1}-n_i-1}}}
=\prod_{i=1}^{r}\frac{(n_{i-1}-n_r-1)!n_i!}{(n_{i-1}-1)!(n_i-n_r)!}
=\prod_{i=1}^{r}\frac{n_i^{\underline{n_r}}}{(n_{i-1}-1)^{\underline{n_r}}}
=\prod_{i=1}^{r} \left( \frac{n_i^{\underline{n_r}}}{n_{i-1}^{\underline{n_r}}}\cdot \frac{n_{i-1}}{n_{i-1}-n_r} \right).
\end{align}

By telescoping 

\begin{align}
\prod_{i=1}^{r} \frac{n_i^{\underline{n_r}}}{n_{i-1}^{\underline{n_r}}}
=\frac{n_r^{\underline{n_r}}}{n_0^{\underline{n_r}}}
=\frac{n_r!(n_0-n_r)!}{n_0!}=\frac{1}{{n_0 \choose n_r}}
=\frac{1}{{m \choose n_r}}.
\end{align}

And 

\begin{align}
\prod_{i=1}^{r} \frac{n_{i-1}}{n_{i-1}-n_r}
=\prod_{i=1}^{r} \frac{1}{1-2^{(i-1-r)/K}}
=\prod_{j=1}^{r} \frac{1}{1-2^{-j/K}}
\le \prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}
\end{align}

%By \url{http://mathworld.wolfram.com/q-PochhammerSymbol.html} We have $\prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}\le 2^{5K}$.

\begin{lemma}
  Let $K\in \mathbb{N}$ and $K\ge 1$. We have $\prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}\le 2^{5K}$.
\end{lemma}

\begin{proof}
  First, we bound the product of first $2K$ terms. Note that $\frac{1}{1-2^{-x}}\le \frac{8}{3x}$ for $0<x\le 2$. Therefore, $\prod_{j=1}^{2K}\frac{1}{1-2^{-j/K}}\le (8/3)^{2K}\cdot \frac{K^{2K}}{(2K)!}\le (8/3)^{2K}\cdot \frac{K^{2K}}{(2K/e)^{2K}}= (4e/3)^{2K}< 2^{4K}$. 
  
  On the other hand, the product of the rest terms  $\prod_{j=2K+1}^{\infty}\frac{1}{1-2^{-j/K}} \le \prod_{j=2K+1}^{\infty}\frac{1}{1-2^{-\lfloor j/K \rfloor}} \le \prod_{i=2}^{\infty}\left( \frac{1}{1-2^{-i}}\right)^K \le \left( \frac{1}{1-\sum_{i=2}^{\infty}2^{-i}}\right)^K=2^K$.
  
  Multiplying two parts proves the lemma.
\end{proof}

Combine together we get $\E(b_r)\ge 1-\frac{5K+1}{\log \frac{1}{\delta}}\ge \frac{2}{5}$. 

\begin{lemma} \label{lemma:bits-saving}
  Let $m=n^{0.99}$. Let $X\in \mathbb{N}$ be a random variable, and $X\le m$. Moreover, $\E(X)\le m-d$. We have $\E(\log {n \choose m}-\log {n \choose X})=\Omega(d \log n)$.
\end{lemma}

\begin{proof}
  \begin{align}
  \log {n \choose m}-\log {n \choose X}
  &=   \log \frac{n!/(m!(n-m)!)}{n!/(X!(n-X)!)} \\
  &=   \sum_{i=1}^{m-X}\log \frac{n-X-i+1}{m-i+1} \\
  &\ge (m-X)\cdot \log \frac{n-X}{m} \\
  &\ge (m-X)\cdot \log n^{1/200}
  \end{align}
  
  Taking expectation on both sides, we get $\E(\log {n \choose m}-\log {n \choose X})\ge \frac{d}{200} \log n$. 
\end{proof}

\begin{theorem}
  $\s = \Omega(\log^2 n\log{\frac{1}{\delta}})$.
\end{theorem}

\begin{proof}
  Let \success be the conjunction of the following events:
  \begin{enumerate}
    \item All $A \cap I_{r,k}$ are non-empty ($r=1,\ldots, R, k=1,\ldots, K$).
    \item $\enc_4$ does not return ``fail'' on input $A$.
  \end{enumerate}
  By Lemma~\ref{lemma:enc_4-sampler-work}, we have $\Pr(\success)\ge \frac{1}{2}$. By Lemma~\ref{lemma:lb-meta}, we have $\s\ge \log (^n_m) - s' -2$. By definition, $s'=\log n + \E(\log (^n_{|B|})|\success)$. By Lemma~\ref{lemma:words-saving} and Lemma~\ref{lemma:bits-saving}, we get $\E(\log (^n_{|B|})|\success)=\log (^n_m)-\Omega(\log^2 n\log{\frac{1}{\delta}})$. 
\end{proof}

\bibliographystyle{alpha}
\bibliography{shortbib}

\end{document}
























