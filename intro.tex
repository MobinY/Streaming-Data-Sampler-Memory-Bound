\section{Introduction}\label{sec:intro}
In turnstile $\ell_0$-sampling, a vector $z\in\R^n$ starts as the zero vector and receives coordinate-wise updates of the form ``$z_i \leftarrow z_i + \Delta$'' for $\Delta\in\{-M,-M+1,\ldots,M\}$. During a query, one must return a uniformly random element from $\supp(x) = \{i : z_i\neq 0\}$. The problem was first defined in \cite{FrahlingIS08}, where a data structure (or ``sketch'') for solving it was used to estimate the Euclidean minimum spanning tree, and to provide $\eps$-approximations of a point set $P$ in a geometric space (that is, one wants to maintain a subset $S\subset P$ such that for any set $R$ in a family of bounded VC-dimension, such as the set of all axis-parallel rectangles, $||R\cap S|/|S| - |R\cap P|/|P|| < \eps$). Sketches for $\ell_0$-sampling were also used to solve various dynamic graph streaming problems in \cite{AhnGM12a} and since then have been crucially used in seemingly {\em every} dynamic graph streaming algorithm, such as for: connectivity, $k$-connectivity, bipartiteness, and minimum spanning tree \cite{AhnGM12a}, subgraph counting, minimum cut, and cut-sparsifier and spanner computation \cite{AhnGM12b}, spectral sparsifiers \cite{AhnGM13}, maximal matching \cite{ChitnisCHM15}, maximum matching \cite{AhnGM12a,BuryS15,Konrad15,AssadiKLY16,ChitnisCEHMMV16,AssadiKL17}, vertex cover \cite{ChitnisCHM15,ChitnisCEHMMV16}, hitting set, $b$-matching, disjoint paths, $k$-colorable subgraph, and several other maximum subgraph problems \cite{ChitnisCEHMMV16}, densest subgraph \cite{BhattacharyaHNT15,McGregorTVV15,EsfandiariHW16}, vertex and hyperedge connectivity \cite{GuhaMT15}, and graph degeneracy \cite{FarachColtonT16}. For an introduction to the power of $\ell_0$-sketches in designing dynamic graph stream algorithms, see the recent survey of McGregor \cite[Section 3]{McGregor14}. Such sketches have also been used outside streaming, such as in distributed algorithms \cite{HegemanPPSS15,Pandurangan0S16} and data structures for dynamic connectivity \cite{KapronKM13,Wang15,GibbKKT15}.

Given the rising importance of $\ell_0$-sampling in algorithm design, a clear task is to understand the exact complexity of this problem. The work \cite{JowhariST11} gave an $\Omega(\log^2 n)$-bit space lower bound for data structures solving the case $M=1$ which fail with constant probability, and otherwise whose query responses are $(1/3)$-close to uniform in statistical distance. They also gave an upper bound for $M \le \mathop{poly}(n)$ with failure probability $\delta$, which in fact gave $\min\{\|z\|_0, \Theta(\log(1/\delta))\}$ uniform samples from the support of $z$, using space $O(\log^2 n \log(1/\delta))$ bits (here $\|z\|_0$ denotes $|\supp(z)|$). Thus we say their data structure actually solves the harder problem of $\ell_0$-sampling$_{\Theta(\log(1/\delta))}$ with failure probability $\delta$, where in $\ell_0$-sampling$_k$ the goal is to recover $\min\{\|z\|_0, k\}$ uniformly random elements, without replacement, from $\supp(z)$.  The upper and lower bounds in \cite{JowhariST11} thus match up to a constant factor for $k = 1$ and $\delta$ a constant.

\paragraph{Universal relation.} The work of \cite{JowhariST11} obtains its lower bound for $\ell_0$-sampling (and some other problems) via reductions from {\em universal relation} ($\ur$). The problem $\ur$ was first defined in \cite{KarchmerRW95} and arose in connection with work of Karchmer and Wigderson on circuit depth lower bounds \cite{KarchmerW90}. For $f:\{0,1\}^n\rightarrow\{0,1\}$, $D(f)$ is the minimum depth of a fan-in $2$ circuit over the basis $\{\neg, \vee, \wedge\}$ computing $f$. Meanwhile, the (deterministic) communication complexity $C(f)$ is defined as the minimum number of bits that need to be communicated in a correct protocol for Alice and Bob to solve the following communication problem: Alice receives $x\in f^{-1}(0)$ and Bob receives $y\in f^{-1}(1)$ (and hence in particular $x\neq y$), and they must both agree on an index $i\in[n]$ such that $x_i\neq y_i$. It is shown in \cite{KarchmerW90} that $D(f) = C(f)$, where they then used this correspondence to show a tight $\Omega(\log^2 n)$ depth lower bound on monotone circuits solving undirected $s$-$t$ connectivity. The problem $\ur$ abstracts away the function $f$ and requires Alice and Bob to agree on the index $i$ only knowing that $x,y\in\{0,1\}^n$ are unequal. The deterministic communication complexity of $\ur$ is nearly completely understood, with upper and lower bounds that match up to an additive $3$ bits, even if one requires an upper bound on the number of rounds \cite{TardosZ97}. Henceforth we also consider a generalized problem $\ur_k$, where the output must be $\min\{k, \|x-y\|_0\}$ distinct indices on which $x, y$ differ. We also use $\ur^{\subset}, \ur_k^{\subset}$ to denote the variants when promised $\supp(y)\subset \supp(x)$. Clearly $\ur, \ur_k$ can only be harder than $\ur^\subset, \ur_k^\subset$, respectively.

More than twenty years after its initial introduction in connection with circuit depth lower bounds, Jowhari et al.\ in \cite{JowhariST11} demonstrated the relevance of $\ur$ in the randomized one-way communication model for obtaining space lower bounds for certain streaming problems, such as various sampling problems and finding duplicates in streams. In particular, if $\randcom^{\rightarrow,pub}_\delta(f)$ denotes the randomized one-way communication complexity of $f$ in the public coin model with failure probability $\delta$, \cite{JowhariST11} showed that the space complexity of \findup{n} with failure probability $\delta$ is at least $\randcom^{\rightarrow,pub}_{\frac 78 + \frac{\delta}8}(\ur)$. In \findup{n}, one is given a length-$(n+1)$ stream of integers in $[n]$, and the algorithm must output some element $i\in[n]$ which appeared at least twice in the stream (note that at least one such element must exist, by the pigeonhole principle). The work \cite{JowhariST11} then showed a reduction demonstrating that any solution to $\ell_0$-sampling with failure probability $\delta$ in turnstile streams immediately implies a solution to \findup{n} with failure probability at most $(1+\delta)/2$ in the same space (and thus the space must be at least $\randcom^{\rightarrow,pub}_{\frac{15}{16} + \frac{\delta}{16}}(\ur)$). The same result is shown for $\ell_p$-sampling for any $p>0$, in which the output index should equal $i$ with probability $|x_i|^p/(\sum_j |x_j|^p)$, and a similar result is shown even if the distribution on $i$ only has to be close to this $\ell_p$-distribution in variational distance (namely, the distance should be bounded away from $1$). It is then shown in \cite{JowhariST11} that $\randcom^{\rightarrow,pub}_\delta(\ur) = \Omega(\log^2 n)$ for any $\delta$ bounded away from $1$. The approach used though unfortunately does not provide an improved lower bound for $\delta\downarrow 0$.

Seemingly unnoticed in \cite{JowhariST11}, we first point out here that the lower bound proof for $\ur$ in that work actually proves the same lower bound for the promise problem $\ur^\subset$. This observation has several advantages. First, it makes the reductions to the streaming problems trivial (they were already quite simple when reducing from $\ur$, but now they are even simpler). Second, a simple reduction from $\ur^\subset$ to sampling problems provides space lower bounds even in the strict turnstile model, and even for the simpler {\em \suppfind{}} streaming problem for which when queried is allowed to return {\em any} element of $\supp(z)$, without any requirement on the distribution of the index output. Both of these differences are important for the meaningfulness of the lower bound. This is because in dynamic graph streaming applications, typically $z$ is indexed by $\binom{n}{2}$ for some graph on $n$ vertices, and $z_e$ is the number of copies of edge $e$ in some underlying multigraph. Edges then are never deleted unless they had previously been inserted, thus only requiring sampler subroutines that are correct with the strict turnstile promise. Also, for every single application mentioned in the first paragraph of Section~\ref{sec:intro} (except for the two applications in \cite{FrahlingIS08}), the known algorithmic solutions which we cited as using $\ell_0$-sampling as a subroutine actually only need a subroutine for the easier \suppfind{} problem. Finally, third and most relevant to the main focus of our current work, the straightforward reductions from $\ur^\subset$ to the streaming problems we are considering here do not suffer any increase in failure probability, allowing us to transfer lower bounds on $\randcom^{\rightarrow,pub}_{\delta}(\ur^\subset)$ for small $\delta$ to lower bounds on various streaming problems for small $\delta$.

We now show simple reductions from $\ur_k^\subset$ to \findupk{n} and \suppfind{k}. In \findupk{n}, the algorithm must report $\min\{k, d\}$ distinct duplicate indices, where $d$ is the actual number of duplicates, and in \suppfind{k} we must report $\min\{k,\|z\|_0\}$ elements in $\supp(z)$. In the claims below, $\delta$ is the failure probability for the considered streaming problem.

\begin{claim}
Any one-pass streaming algorithm for \findupk{n} must use $(\randcom^{\rightarrow,pub}_{\delta}(\ur_k^\subset) - \lceil \log_2 n\rceil)$ bits of space.
\end{claim}
\begin{proof}
  We reduce from $\ur_k^\subset$. Suppose there were a space-$S$ algorithm $\mathcal{A}$ for \findupk{n}. Alice creates a stream consisting of all elements of $\supp(x)$ and runs $\mathcal{A}$ on those elements, then sends the memory contents of $\mathcal{A}$ to Bob as well as $|\supp(x)|$. Bob then continues running $\mathcal{A}$ on $n+1-|\supp(x)|$ arbitrarily chosen elements of $[n]\backslash\supp(y)$. Then there a positive number of duplicates in the resulting concatenated stream, and all duplicates $i$ satisfy $x_i\neq y_i$.
\end{proof}

\begin{claim}
Any one-pass streaming algorithm for \suppfind{k} in the strict turnstile model must use $\randcom^{\rightarrow,pub}_{\delta}(\ur_k^\subset)$ bits of space, even if promised that $z\in\{0,1\}^n$ at all points in the stream.
\end{claim}
\begin{proof}
This is again via reduction from $\ur_k^\subset$. Let $\mathcal{A}$ be a space-$S$ algorithm for \suppfind{k} in the strict turnstile model. For each $i\in\supp(x)$, Alice sends the update $z_i \leftarrow z_i + 1$ to $\mathcal{A}$. Alice then sends the memory contents of $\mathcal{A}$ to Bob. Bob then for each $i\in\supp(y)$ sends the update $z_i\leftarrow z_i - 1$ to $\mathcal{A}$. Now note that $z$ is exactly the indicator vector of the set $\{i : x_i\neq y_i\}$.
\end{proof}

\begin{claim}
Any one-pass streaming algorithm for $\ell_p$-sampling for any $p\ge 0$ in the strict turnstile model must use $\randcom^{\rightarrow,pub}_{\delta}(\ur_k^\subset)$ bits of space, even if promised $z\in\{0,1\}^n$ at all points in the stream.
\end{claim}
\begin{proof}
This is via straightforward reduction from \suppfind{k}, since reporting $\min\{k,\|z\|_0\}$ elements of $\supp(z)$ satisfying some distributional requirements is only a harder problem than finding {\em any} $\min\{k,\|z\|_0\}$ elements of $\supp(z)$.
\end{proof}


% The lower bound proof for $\randcom^{\rightarrow,pub}_{\delta}(\ur)$ in \cite{JowhariST11} was via reduction from a problem known as {\em augmented indexing} ($\mathbf{AugIndex}$) \cite{MiltersenNSW98}. In this problem, Alice receives $x\in\{0,1\}^n$ and Bob receives $i\in[n]$ together with $x_1,\ldots,x_{i-1}$ and Bob must output $x_i$ after receiving a single message from Alice. The work \cite{MiltersenNSW98} showed $\randcom^{\rightarrow,pub}_{1/3}(\mathbf{AugIndex}) = \Omega(n)$. This was extended to larger domains, i.e.\ in $\mathbf{AugIndex}(k)$ we have $x\in[k]^n$, in \cite{ErgunJS10,JayramW13}, where it was shown that $\randcom^{\rightarrow,pub}_{\delta}(\mathbf{AugIndex}(k)) = \Omega(n\log k)$ for any $\delta < 1 - 3/k$.

The reductions above thus raise the question: what is the asymptotic behavior of $\randcom^{\rightarrow,pub}_\delta(\ur_k^\subset)$?

\paragraph{Our main contribution:} We prove for any $\delta$ bounded away from $1$ and $k\in[n]$, $\randcom^{\rightarrow,pub}_\delta(\ur_k^\subset) = \Theta(\min\{n, t\log^2(n/t)\})$ where $t = \max\{k,\log(1/\delta)\}$. Given known upper bounds in \cite{JowhariST11}, our lower bounds are optimal for \findup{n}, \suppfind{}, and $\ell_p$-sampling for any $0\le p<2$ for nearly the full range of $n, \delta$ (namely, for $\delta > 2^{-n^{.99}}$). Also given an upper bound of \cite{JowhariST11}, our lower bound is optimal for $\ell_0$-sampling$_k$ for nearly the full range of parameters $n, k, \delta$ (namely, for $t < n^{.99}$). Previously no lower bounds were known in terms of $\delta$ (or $k$). Our main theorem:

%\bigskip

\begin{theorem}\label{thm:main}
For any $\delta$ bounded away from $1$ and $1\le k\le n$, $\randcom^{\rightarrow,pub}_\delta(\ur_k^\subset) = \Theta(\min\{n, t\log^2(n/t)\})$.
\end{theorem}


% \paragraph{Our contribution:} We completely resolve all four issues discussed in the last paragraph. We show that any solution to the even the easier \suppfind{k} problem requires $\Omega(\min\{n, t \log^2(n/t)\})$ bits of space for $t = \max\{k, \log(1/\delta)\}$, even in the easier strict turnstile setting. Also, as in \cite{JowhariST11}, our lower bound does not require large weights and still holds even if it is promised that $x\in\{0,1\}^n$ at all points in the stream. Given the $O(t\log^2 n)$-bit upper bound of \cite{JowhariST11}, our lower bound is optimal for nearly the full range of $k, \delta, n$ (it is optimal as long as, say, $t\cdot \log^2(n/t)\le n^{.99}$ --- note there is always a trivial $O(n \log n)$-bit algorithm by storing $x$ in memory explicitly). Furthermore, since our lower bound holds for the easier \suppfind{} problem, it holds against $\ell_p$-samplers for every $p$ (in which case one wants to sample $i$ with probability $(1\pm\eps)|x_i|^p/\|x\|_p^p$), or more generally against any variant that specifies requirements on the output distribution within the support. Also due to upper bounds provided in \cite{JowhariST11}, our lower bound is optimal for $\ell_p$-sampling for constant $\eps$ for any $0\le p<2$. \TODO{figure out if any of the previous applications needed small $\delta$, or \suppfind{k} for $k > 1$, then say so} All our lower bounds are corollaries of a tight lower bound for a communication problem $\ur_k$, which we now proceed to describe, which turns out to also simply imply an optimal lower bound for another streaming problem: finding duplicates in streams.

% Also, as an immediate corollary, we completely resolve the complexity of finding duplicates in data streams. In this problem the stream is a sequence of integers $i_1 i_2 \cdots i_m \in [n]$, and we would like to report an index $j\in[n]$ which appeared at least twice in the stream (if no index appeared at least twice, we should return \textsf{NULL}). Again, the data structure may be randomized and behave arbitrarily with probability $\delta$. An algorithm solving this problem using $O(\log(1/\delta)\log^2 n)$ bits is given in \cite{JowhariST11}, with a lower bound of $\Omega(\log^2 n)$ bits also proven there for constant failure probability.

% \begin{theorem}\label{thm:duplicate}
% Any algorithm for finding a duplicate in a stream with failure probability $\delta$ must use space $\Omega(\min\{n, \log(1/\delta) \log^2(n / \log(1/\delta))^2\})$ bits. In particular, this is $\Omega(\log(1/\delta)\log^2 n)$ bits for $2^{-n^{.99}} \le \delta < 1$.
% \end{theorem}
% \begin{proof}
% We prove this lower bound via a simple reduction from \suppfind{1} in the strict turnstile model. Suppose the underlying vector being streamed in \suppfind{1} is $x\in\R^n$. Before processing the stream, we 
% \end{proof}


\subsection{Related work}
The question of whether $\ell_0$-sampling is possible in low memory in turnstile streams was first asked in \cite{CormodeMR05}. The first solution was given in \cite{FrahlingIS08}, where it was applied to approximate the cost of the Euclidean minimum spanning tree of a subset $S$ of a discrete geometric space subject to insertions and deletions. The algorithm given there used space $O(\log^3 n)$ bits to achieve failure probability $1/\mathop{poly}(n)$ (though it is likely that the space could be improved to $O(\log^2 n\log\log n)$ with a worse failure probability, by replacing a subroutine used there with a more recent $\ell_0$-estimation algorithm of \cite{KaneNW10}). As mentioned, the currently best known upper bound solves $\ell_0$-sampling$_k$ using $O(t\log^2 n)$ bits \cite{JowhariST11}, which Theorem~\ref{thm:main} shows is tight.

For $\ell_p$-sampling as defined above, the first work to realize its importance came even earlier than for $\ell_0$-sampling: \cite{CoppersmithK04} showed that an $\ell_2$-sampler using small memory would lead to a nearly space-optimal streaming algorithm for multiplicatively estimating $\|x\|_3$ in the turnstile model, but did not know how to implement such a data structure. The first implementation was given in \cite{MonemizadehW10}, where they achieved space $\mathop{poly}(\eps^{-1}\log n)$ for failure probability $1/\mathop{poly}(n)$. For $1\le p\le 2$ the space was improved to $O(\eps^{-p}\log^3 n)$ bits for constant failure probability \cite{AndoniKO11}. In \cite{JowhariST11} this bound was improved to $O(\eps^{-\max\{1,p\}}\log(1/\delta)\log^2 n)$ bits for failure probability $\delta$ when $0<p<2$ and $p\neq 1$. For $p=1$ the space bound achieved by \cite{JowhariST11} was a $\log(1/\eps)$ factor worse: $O(\eps^{-1}\log(1/\eps)\log(1/\delta)\log^2 n)$ bits.

\TODO{cite more previous work, like \cite{GopalanR09} on finding duplicates}

\section{Overview of techniques}
We describe our proof of Theorem~\ref{thm:main}. For the upper bound, \cite{JowhariST11} achieved $O(t\log^2n)$, but in the appendix we show that slight modifications to their approach yield $O(\min\{n,t\log^2(n/t)\})$ bits. Our main contribution is in proving an improved lower bound. Our lower bound proof is split into two parts: we show $\randcom^{\rightarrow,pub}_\delta(\ur^\subset) = \Omega(\log(1/\delta)\log^2 n)$ and $\randcom^{\rightarrow,pub}_{\frac 12}(\ur_k^\subset)=\Omega(k\log^2n)$ separately. We first describe the former, which is the more technically challenging half.

We prove the lower bound via an encoding argument. Fix $m$. A randomized encoder is given a set $S\subset[n]$ with $|S| = m$ and must output an encoding $\enc(S)$, and a decoder sharing public randomness with the encoder must then be able to recover $S$ given access only to $\enc(S)$. We consider such schemes in which the decoder must succeed with probability $1$, and the encoding length is a random variable. Clearly any such encoding must use $\Omega(\log(^n_m)) = \Omega(m\log n)$ bits in expectation for some $S$ (we will choose $m < n^{.99}$).

There is a natural, but sub-optimal approach to using a public-coin one-way protocol $\mathcal{A}$ for $\ur^\subset$ to devise such an encoding/decoding scheme.  The encoder pretends to be Alice with input $x$ being the indicator set of $S$, then lets $\enc(S)$ be the message $M$ Alice would have sent to Bob. The decoder attempts to recover $S$ by iteratively pretending to be Bob $m$ times, initially pretending to have input $y=0\in\{0,1\}^n$, then iteratively adding elements found in $S$ to $y$'s support. Henceforth let $\mathbf{1}_T\in\{0,1\}^n$ denote the indicator vector of a set $T\subset[n]$.

\begin{algorithm}[H] 
  \caption{Simple Decoder.} \label{algo:wrong}
  \begin{algorithmic}[1]
    \Procedure{$\dec$}{$M$}
    \State $T\leftarrow \emptyset$
    \For {$r=1,\ldots,m$} 
      \State Let $i$ be Bob's output upon receiving message $M$ from Alice when Bob's input is $\mathbf{1}_T$
      \State $T \leftarrow S \cup\{i\}$
    \EndFor
    \State \Return $S$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

One might hope to say that if the original failure probability were $\delta < 1/m$, then by a union bound, with constant probability every iteration succeeds in finding a new element of $S$ (or one could even first apply some error-correction to $x$ so that the decoder could recover $S$ even if only a constant fraction of iterations succeeded). The problem with such thinking though is that this decoder chooses $y$'s adaptively! To be specific, $\mathcal{A}$ being a correct protocol means
\begin{equation}
\forall x,y\in\{0,1\}^n,\ \Pr_s(\mathcal{A}\text{ is correct on inputs }x,y) \ge 1-\delta , \label{eqn:correct}
\end{equation}
where $s$ is the public random string that both Alice and Bob have access to. The issue is that even in the second iteration (when $r=2$), Bob's ``input'' $\mathbf{1}_T$ {\em depends on $s$}, since $T$ depends on the outcome of the first iteration! Thus the guarantee of \eqref{eqn:correct} does not apply.

One way around the above issue is to realize that as long as every iteration succeeds, $T$ is always a subset of $S$. Thus it suffices for the following event $\mathcal{E}$ to occur: $\forall T\subset S,\ \mathcal{A}\text{ is correct on inputs }\mathbf{1}_S, \mathbf{1}_T$. Then $\Pr_s(\neg \mathcal{E}) \le 2^m\delta$ by a union bound, which is at most $1/2$ for $m = \lfloor \log_2(1/\delta)\rfloor - 1$. We have thus just shown that $\randcom^{\rightarrow,pub}_\delta(\ur^\subset) = \Omega(\min\{n, \log(^n_m)\}) = \Omega(\min\{n, \log(1/\delta)\log \frac n{\log(1/\delta)}\})$.

The key observation for improvement is as follows. Assume that every iteration of the decoder succeeds, so $T\subseteq S$ always. Now observe Eq.~\eqref{eqn:correct} {\em does} imply that for any $x$, the average public random string $s$ does cause the protocol to succeed on most of these possibilities for $y$ (namely at least a $1-\delta$ fraction).
 % If one then performs wishful thinking to hope that a $(1-\delta)$-fraction of {\em iterations} actually succeed, one could proceed as follows: since the encoder and decoder share a common random string, the encoder can simulate the decoder and know exactly which iterations would fail (say, $q\ll m$ iterations). The encoder could then include in the encoding a description of which iterations those are, using $\lceil\log_2 q\rceil + \lceil\log(^m_q) \rceil$ bits, and also include $q$ random elements from $S$ as part of its message, for the decoder to use instead of Bob's output in the failed iterations. The extra overhead would be $O(q\log n) \ll m\log n$ bits, and thus minor.
Our improved decoder roughly proceeds as follows. The decoder will again iteratively try to recover elements of $S$ as before. We will give up though on having $m$ iterations and hoping for all (or even most) of them to succeed. Instead, we will only have $R = \Theta(\log(1/\delta)\log n)$ iterations, and our aim is for the decoder to succeed in finding a new element in $S$ for at least a constant fraction of these $R$ iterations. Simplifying things for a moment, let us pretend for now that all $R$ iterations do succeed in finding a new element. $\enc(S)$ will then be Alice's message $M$, together with a random subset $B$ of $m-R$ elements of $S$, explicitly written down. If the decoder can then recover these $R$ remaining elements, this then implies the decoder has recovered $S$, and thus we must have $|M| = \Omega(R\log \frac nm)$ as desired. The decoder proceeds as follows. Just as before, initially the decoder starts with $T = \emptyset$ and lets $i$ be the output of Bob and adds it to $T$. Then in iteration $r$, before proceeding to the next iteration, the decoder randomly picks some number $n_r$ elements from $B$ and adds them into $T$! These extra elements being added to $T$ should be viewed ``random noise'' to mask information about the random string $s$ used by $\mathcal{A}$. For intuition, as an extreme example, suppose the iteration $r=1$ succeeds in finding some $i\in S$. If the decoder were then to add $i$ to $T$, as well as $\approx m/2$ random elements from $B$ to $T$, then the resulting $T$ reveals only $\approx 1$ bit of information about $i$ (and hence about $s$). This is as opposed to the $\log n$ bits about $i$ that $T$ would have revealed if the masking were not performed. Thus the next query in round $r=2$, although correlated with $s$, has very weak correlation after masking and we thus might hope for it to succeed. This intuition is captured in the following lemma, which we prove in Section~\ref{sec:optimal-lb}:
\begin{lemma}\label{lem:information}
  Consider $f$: $\{0,1\}^b\times \{0,1\}^q\rightarrow \{0,1\}$ and $X\in\{0,1\}^b$ uniformly random. If $\forall y\in \{0,1\}^m,\ \Pr(f(X,y)=1)\le \delta$ where $0<\delta<1$, then for any random variable $Y$ supported on $\{0,1\}^q$,
  \begin{align}
    \Pr(f(X,Y)=1)\le \frac{I(X;Y)+1}{\log \frac{1}{\delta}},
  \end{align}
  where $I(X;Y)$ is the mutual information (in bits) between $X$ and $Y$.
\end{lemma}
Fix some $x\in\{0,1\}^n$. One should imagine here that $f(X,y)$ is $1$ iff $\mathcal{A}$ fails when Alice has input $x$ and Bob has input $y$ in a $\ur^\subset$ instance, and the public random string is $X=s$. Then the lemma states that if $y=Y$ is not arbitrary, but rather random (and correlated with $X$), then the failure probability of the protocol is still bounded as long as the mutual information between $X$ and $Y$ is bounded. It is also not hard to see that this lemma is sharp as written. Consider the case $x,y\in[n]$, and $f(x,y) = 1$ iff $x = y$. Then if $X$ is uniform, for all $y$ we have $\Pr(f(X,y) = 1) = 1/n$. Now consider the case where $Y$ is random and equal to $X$ with probability $t/\log n$ and is uniform in $[n]$ with probability $1 - t/\log n$. Then in expectation $Y$ reveals $t$ bits of $X$, so that $I(X;Y) = t$. It is also not hard to see that $\Pr(f(X,Y) = 1) \approx t/\log n$.

In light of the strategy stated so far and Lemma~\ref{lem:information}, the path forward is clear: at each iteration $r$, we should add enough random masking elements to $T$ to keep the mutual information between $T$ and all previously added elements below $\approx \frac 12 \log \frac 1{\delta}$. Then we expect a constant fraction of iterations succeed. The encoder knows which iterations do not succeed since he shares public randomness with the decoder (and can thus simulate him), so he can simply tell the decoder which rounds are the failed ones, then explicitly include in $M$ correct new elements of $S$ for the decoder to use in the place of Bob's wrong output in those rounds. It turns out the right number of random masking elements to add after iteration $r$ to keep the mutual information sufficiently small is $m(2^{-c\frac{r-1}{\log(1/\delta)}} - 2^{-c\frac{r}{\log(1/\delta)}})$, so that $T$ has size $m(1 - 2^{-c\frac r{\log(1/\delta)}})$ right before iteration $r+1$ starts. This is implied by our Lemma~\ref{lemma:mutual-entropy-bound}. This also implies that the number of iterations we can have before $T$ becomes all of $S$, i.e.\ has size $m$, is $R = \Theta(\log(1/\delta)\log(\frac{m}{\log(1/\delta)}))$. We then set $m = \sqrt{n\log(1/\delta)}$, so that $\randcom^{\rightarrow,pub}_\delta(\ur^\subset) = \Omega(|R|\log \frac nm) = \Omega(\min\{n, \log(1/\delta)\log^2 \frac n{\log(1/\delta)}\})$ as desired.

The argument for lower bounding $\randcom^{\rightarrow,pub}_\delta(\ur_k^\subset)$ is a bit simpler, and in particular does not need rely on Lemma~\ref{lem:information}. Both the idea and rigorous argument can be found in Section~\ref{sec:k-samples-lb}, but again the idea is to use a protocol for this problem to encode appropriately sized subsets of $[n]$.


% As mentioned, we show space lower bounds for maintaining a \suppfind{} data structure for a {\em binary} vector. That is, our lower bound holds even if at all times in the stream we are guaranteed $x\in \{0,1\}^n$. Our lower bounds are based on communication complexity in the public random coin model. Alice wants to send Bob a uniform random set $A\subseteq [n]$ of size $m$ (Bob knows $m$, but the random source generating $A$ is independent of the random source accessible to Bob). The one-way communication problem is: Alice sends some message to Bob, and Bob is required to recover $A$ completely. Any randomized protocol that succeeds with probability $1$ requires Alice to send at least $\log (^n_m)$ bits in expectation.

% We consider the following protocol. Alice attaches (the memory of) a support-finder \samp in the message. The support-finder uses public random coins as its random source, so that \samp will behave the same for Alice and for Bob as long as the updates are all the same. Alice will insert all the items in $A$ into \samp and send the memory footprint of \samp to Bob as a message. In addition, Alice will send a subset $B\subseteq A$ to Bob, so that together with $B$ and \samp, Bob is able to recover $A$ with good probability based on some protocol they have agreed on. 
As mentioned above, our lower bounds will use protocols for $\ur^\subset$ and $\ur^\subset_k$ to estabalish protocols for encoding subsets of some fixed size $m$ of $[n]$. These encoders always consist of some message $M$ Alice would have sent in a $\ur^\subset$ or $\ur^\subset_k$ protocol, together with a random subset $B\subset S$ (using $\lceil \log_2|B|\rceil + \lceil\log{n\choose |B|}\rceil$ bits, to represent both $|B|$ and the set $B$ itself). Here $|B|$ is a random variable. These encoders are thus {\em Las Vegas}: the length of the encoding is a random variable, but the encoder/decoder always succeed in compressing and recovering the subset. The final lower bounds then come from the following simple lemma. 

\begin{lemma} \label{lemma:lb-meta}
  Let $\s$ denote the number of bits used by the $\ur^\subset$ or $\ur^\subset_k$ protocol, and let $\s'$ denote the expected number of bits to represent $B$. Then $(1+\s+\s') \ge \log (^n_m)$. In particular, $s \ge \log(^n_m) - s' - 1$.
\end{lemma}

% We consider the range of failure probability $\delta$ to be 
% \begin{align} \label{eqn:delta-range}
% 2^{-n^{c_1}}<\delta<c_2,
% \end{align}
% where $c_1=0.9$ and $c_2=2^{-10}$. In fact, our lower bound applies for the range of $\delta$ where $c_1$ and $c_2$ are any constants smaller than $1$. 

Section~\ref{sec:optimal-lb} provides the full details of the proof that $\randcom^{\rightarrow,pub}_\delta(\ur^\subset) = \Omega(\min\{n, \log^2(\frac n{\log(1/\delta)}) \log \frac{1}{\delta}\})$. We extend our results in Section~\ref{sec:k-samples-lb} to $\ur_k^\subset$ for $k\ge 1$, proving a lower bound of $\Omega(k\log^2(n/k))$ communication even for constant failure probability.
