\section{Introduction}
In turnstile $\ell_0$-sampling, a vector $x\in\R^n$ starts as the zero vector and receives coordinate-wise updates of the form ``$x_i \leftarrow x_i + \Delta$'' for $\Delta\in\{-M,-M+1,\ldots,M\}$. During a query, one must return a uniformly random element from $\supp(x) = \{i : x_i\neq 0\}$. The problem was first defined in \cite{FrahlingIS08}, where a data structure (or ``sketch'') for solving it was used to estimate the Euclidean minimum spanning tree, and to provide $\eps$-approximations of a point set $P$ in a geometric space (that is, one wants to maintain a subset $S\subset P$ such that for any set $R$ in a family of bounded VC-dimension, such as the set of all axis-parallel rectangles, $||R\cap S|/|S| - |R\cap P|/|P|| < \eps$). Sketches for $\ell_0$-sampling were also used to solve various dynamic graph streaming problems in \cite{AhnGM12a} and since then have been crucially used in seemingly {\em every} dynamic graph streaming algorithm, such as for: connectivity, $k$-connectivity, bipartiteness, and minimum spanning tree \cite{AhnGM12a}, subgraph counting, minimum cut, and cut-sparsifier and spanner computation \cite{AhnGM12b}, spectral sparsifiers \cite{AhnGM13}, maximal matching \cite{ChitnisCHM15}, maximum matching \cite{AhnGM12a,BuryS15,Konrad15,AssadiKLY16,ChitnisCEHMMV16,AssadiKL17}, vertex cover \cite{ChitnisCHM15,ChitnisCEHMMV16}, hitting set, $b$-matching, disjoint paths, $k$-colorable subgraph, and several other maximum subgraph problems \cite{ChitnisCEHMMV16}, densest subgraph \cite{BhattacharyaHNT15,McGregorTVV15,EsfandiariHW16}, vertex and hyperedge connectivity \cite{GuhaMT15}, and graph degeneracy \cite{FarachColtonT16}. For an introduction to the power of $\ell_0$-sketches in designing dynamic graph stream algorithms, see the recent survey of McGregor \cite[Section 3]{McGregor14}. Such sketches have also been used outside streaming, such as in distributed algorithms \cite{HegemanPPSS15,Pandurangan0S16} and data structures for dynamic connectivity \cite{KapronKM13,Wang15,GibbKKT15}.

Given the rising importance of $\ell_0$-sampling in algorithm design, a clear task is to understand the exact complexity of this problem. The work \cite{JowhariST11} gave an $\Omega(\log^2 n)$-bit space lower bound for data structures solving the case $M=1$ which fail with constant probability, and otherwise whose query responses are $(1/3)$-close to uniform in statistical distance. They also gave an upper bound for $M \le \mathop{poly}(n)$ with failure probability $\delta$, which in fact gave $\min\{\|x\|_0, \Theta(\log(1/\delta))\}$ uniform samples from the support of $x$, using space $O(\log^2 n \log(1/\delta))$ bits (here $\|x\|_0$ denotes $|\supp(x)|$). Thus we say their data structure actually solves the harder problem of $\ell_0$-sampling$_{\Theta(\log(1/\delta))}$ with failure probabliity $\delta$, where in $\ell_0$-sampling$_k$ the goal is to recover $\min\{\|x\|_0, k\}$ uniformly random elements, without replacement, from $\supp(x)$. Note that their upper and lower bounds match up to a constant factor for $k = 1$ and $\delta$ a constant.

The lower bound proven in \cite{JowhariST11} leaves four items to be better understood. First, their lower bound is not sensitive to $\delta$. Second, no lower bound is given in terms of $k$ for $\ell_0$-sampling$_k$. Third, their lower bound is proven against data structures that must be correct in the so-called {\em general turnstile} setting, which is the setting in which $x_i$ are allowed to be negative. This is in the contrast to the {\em strict turnstile} setting, in which the data structure is promised that, although some updates $\Delta$ may be negative, no $x_i$ will ever be negative. This is particular relevant for dynamic graph streaming applications, since in those applications typically $x$ is indexed by $\binom{n}{2}$ for some graph on $n$ vertices, and $x_e$ is the number of copies of edge $e$ in some underlying multigraph. Edges then are never deleted unless they had previously been inserted, thus not requiring $\ell_0$-sampler subroutines that must be correct in general turnstile. Finally, fourth and most importantly, their lower bound is for $\ell_0$-sampling, whereas for every single application mentioned in the first paragraph (except for the two applications in \cite{FrahlingIS08}), the known algorithmic solutions using $\ell_0$-sampling as a subroutine actually need a subroutine for an {\em easier} problem, which we call {\em \suppfind{k}}. Whereas in $\ell_0$-sampling$_k$ a query asks for $k$ uniformly random elements from $\supp(x)$, in \suppfind{k} the response to a query is allowed to be a set of {\em any} $\min\{\|x\|_0, k\}$ elements from $\supp(x)$ with probability $1-\delta$, and again with probability $\delta$ the response may be arbitrary. The \suppfind{k} problem is obviously easier than $\ell_0$-sampling$_k$, since the output indices from $\supp(x)$ need not be uniformly random, or even random at all. Despite this, the best known algorithm for solving \suppfind{k} with failure probability $\delta$ is to simply solve $\ell_0$-sampling$_k$ with failure probability $\delta$, which by the last paragraph is thus $O((k+\log(1/\delta))\log^2 n)$ bits. Meanwhile, from the lower bound side, no lower bound for support-finding$_k$ was known beyond the trivial $\Omega(\log\binom nk) = \Omega(k\log(n/k))$ bits required to write down a set of size $k$. This gap in our understanding is despite the fact that support-finding, not $\ell_0$-sampling, is the more relevant subroutine for most applications.


\paragraph{Our contribution:} We completely resolve all four questions discussed in the last paragraph. We show that any solution to the even the easier \suppfind{k} problem requires space $\Omega(\min\{n, t \log^2(n/t)\})$ bits for $t = \max\{k, \log(1/\delta)\}$, even in the easier strict turnstile setting. Also, as in \cite{JowhariST11}, our lower bound does not require large weights and still holds even if it is promised that $\|x\|_\infty \le 1$ at all points in the stream. Given the $O(t\log^2 n)$-bit upper bound of \cite{JowhariST11}, our lower bound is optimal for nearly the full range of $k, \delta, n$ (it is optimal as long as, say, $t\cdot \log^2(n/t)\le n^{.99}$ --- note there is always a trivial $O(n \log n)$-bit algorithm by storing $x$ in memory explicitly). Furthermore, since our lower bound holds for the easier \suppfind{} problem, it holds against $\ell_p$-samplers for every $p$ (in which case one wants to sample $i$ with probability $(1\pm\eps)|x_i|^p/\|x\|_p^p$), or more generally against any variant that specifies requirements on the output distribution within the support. Also due to upper bounds provided in \cite{JowhariST11}, our lower bound is optimal for $\ell_p$-sampling for constant $\eps$ for any $0<p<2$.

% Also, as an immediate corollary, we completely resolve the complexity of finding duplicates in data streams. In this problem the stream is a sequence of integers $i_1 i_2 \cdots i_m \in [n]$, and we would like to report an index $j\in[n]$ which appeared at least twice in the stream (if no index appeared at least twice, we should return \textsf{NULL}). Again, the data structure may be randomized and behave arbitrarily with probability $\delta$. An algorithm solving this problem using $O(\log(1/\delta)\log^2 n)$ bits is given in \cite{JowhariST11}, with a lower bound of $\Omega(\log^2 n)$ bits also proven there for constant failure probability.

% \begin{theorem}\label{thm:duplicate}
% Any algorithm for finding a duplicate in a stream with failure probability $\delta$ must use space $\Omega(\min\{n, \log(1/\delta) \log^2(n / \log(1/\delta))^2\})$ bits. In particular, this is $\Omega(\log(1/\delta)\log^2 n)$ bits for $2^{-n^{.99}} \le \delta < 1$.
% \end{theorem}
% \begin{proof}
% We prove this lower bound via a simple reduction from \suppfind{1} in the strict turnstile model. Suppose the underlying vector being streamed in \suppfind{1} is $x\in\R^n$. Before processing the stream, we 
% \end{proof}

\subsection{Related work}
As mentioned above, $\ell_0$-sampling was first studied in 

\section{Overview of techniques}
Note that updates are independent of the randomness used in the sampler. That is, for the purpose of proving a lower bound, we assume an oblivious adversary. 

As mentioned, we show space lower bounds for maintaining a \suppfind{} data structure for a {\em binary} vector. That is, at any time, we are guaranteed that $x\in \{0,1\}^n$. Our lower bounds are based on communication complexity in the public random coin model. Alice wants to send Bob a uniform random set $A\subseteq [n]$ of size $m$ (Bob knows $m$, but the random source generating $A$ is independent of the random source accessible to Bob). The one-way communication problem is: Alice sends some message to Bob, and Bob is required to recover $A$ completely. Since the randomness in $A$ contains $\log (^n_m)$ bits of information, any randomized protocol that succeeds with probability $1$ requires at least $\log (^n_m)$ expected bits. 

We consider the following protocol. Alice attaches (the memory of) a support-finder \samp in the message. The support-finder uses public random coins as its random source, so that \samp will behave the same for Alice and for Bob as long as the updates are all the same. Alice will insert all the items in $A$ into \samp and send the memory footprint of \samp to Bob as a message. In addition, Alice will send a subset $B\subseteq A$ to Bob, so that together with $B$ and \samp, Bob is able to recover $A$ with good probability based on some protocol they have agreed on. 
 
Now we turn the previous protocol into a new one without any failure. Let \success denote the event (or a subset of the event) that Bob successfully recovers $A$ (note that Alice can simulate Bob, so she knows exactly when \success happens). If \success happens Alice will send Bob a message starting with a $1$, followed by (the memory of) \samp, then followed by a naive encoding (explained later) of $B$; otherwise, Alice will send a message starting with a $0$, followed by a naive encoding of $A$. We say the {\em naive encoding} of a set $S\subseteq [n]$ is an integer (expressed in binary) in $[{n \choose |S|}]$ together with $|S|$ (taking $\log n$ bits). We drop the size of the set if it is known by the receiver.

\begin{lemma} \label{lemma:lb-meta}
  Let $\s$ denote the space (in bits) used by a sampler with failure probability at most $\delta$. Let $\s'$ denote the expected number of bits to represent $B$ conditioned on \success (if we need to send some extra auxiliary information, we will also count it into $\s'$). We have 
  
  \begin{align}
  (1+\s+\s')\cdot \Pr(\success)+(1+\log(^n_m) \cdot (1-\Pr(\success)) \ge \log (^n_m).
  \end{align} 
  
  If $\Pr(\success)\ge 1/2$, we have 
  
  \begin{align} \label{eqn:lb-meta}
  \s\ge \log (^n_m) - \s' - 2.
  \end{align} 
\end{lemma}

% We consider the range of failure probability $\delta$ to be 
% \begin{align} \label{eqn:delta-range}
% 2^{-n^{c_1}}<\delta<c_2,
% \end{align}
% where $c_1=0.9$ and $c_2=2^{-10}$. In fact, our lower bound applies for the range of $\delta$ where $c_1$ and $c_2$ are any constants smaller than $1$. 

In Section~\ref{sec:simple-lb} we give a lower bound of $\Omega(\log(\frac n{\log(1/\delta)}) \log \frac{1}{\delta})$ bits. This illustrates some key ideas of our framework. Then we show a lower bound of $\Omega(\log^2(\frac n{\log(1/\delta)}) \log \frac{1}{\delta})$ bits in Section~\ref{sec:optimal-lb}.
We extend our results in Section~\ref{sec:k-samples-lb} to \suppfind{k} for $k\ge 1$ proving a lower bound of $\Omega(k\log^2(n/k))$ bits for constant failure probability.

\begin{remark}
  Because the space lower bound in this note is proven via communication complexity under public random coin model, it also applies to non-uniform models of computation such as circuits and branching programs.  \TODO{what does this mean?}
\end{remark}