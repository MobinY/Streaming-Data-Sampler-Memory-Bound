\section{Communication Lower Bound for $\ur^\subset$} \label{sec:optimal-lb}

%In the previous section we have shown how to extract $\Theta(\log \frac{1}{\delta})$ words of information from the sketch by sequentially peeling off elements. 
%Because the number of bad events we need to union-bound increases exponentially, the approach cannot obtain more elements. 

Consider a protocol $\mathcal{P}$ for $\ur^\subset$ with failure probability $\delta$, operating in the one-way public coin model. When Alice's input is $x$ and Bob's input is $y$, Alice sends $\sketch(x)$ to Bob, and Bob outputs $\query(\sketch(x), y)$, which with probability at least $1-\delta$ is in $\supp(x-y)$. As mentioned in Section~\ref{sec:overview}, we use $\mathcal{P}$ as a subroutine in a scheme for encoding/decoding elements of $\binom{[n]}m$ for $m = \lfloor \sqrt{n\log(1/\delta)}\rfloor$. In this section, we assume $\log \frac 1{\delta} \le n/64$, since for larger $n$ we have an $\Omega(n)$ lower bound.

% Let us consider failure probability of $\query(\sketch(\mathbf{1}_S), \mathbf{1}_T)$ where $T\subset S$ in terms of information leak, which is defined as the mutual information (conditioned on $S$) between $T$ and random source used in the $\ur^\subset$-protocol. We show in this section that the failure probability is upper-bounded by information leak divided by $\log \frac{1}{\delta}$. Therefore, in order to invoke $\query$ with good success probability, it is sufficient to control information leak.

%  Our key technique is noise injection: after obtaining an element from $S\backslash T$ via the $\ur^\subset$-protocol, we not only add the element to $T$, but also randomly add roughly a $1/K$ fraction of elements in $S\backslash T$ to $T$. 
% We prove that for every round, the mutual information between the randomness used by $\mathcal{P}$ and the next inputs fed to $\mathcal{P}$ by the encoding scheme is $O(K)$.  
% By setting $K=\Theta(\log \frac{1}{\delta})$ and $m=\sqrt{n\log\frac{1}{\delta}}$, we can proceed in $\Theta(\frac{\log (m/K)}{\log (1 +1/K)}) = \Theta(\log\frac{1}{\delta}\log\frac{n}{\log(1/\delta)})$ rounds, and thus in expectation obtain $\Theta(\log\frac{1}{\delta}\log\frac{n}{\log(1/\delta)})$ elements in $S$.
% Moreover, each element contains $\Theta(\log(n/m))=\Theta(\log \frac{n}{\log (1/\delta)})$ bits of information, so we get $\Theta(\log \frac{1}{\delta}\log^2 \frac{n}{\log (1/\delta)}) $ bits of information from $\sketch(\mathbf{1}_S)$. 

\subsection{Encoding/decoding scheme}
We now describe our encoding/decoding scheme $(\enc, \dec)$ for elements in ${[n] \choose m}$, which uses $\mathcal{P}$ in a black-box way. The parameters shared by $\enc$ and $\dec$ are given in Algorithm~\ref{algo:para}.

As discussed in Section~\ref{sec:overview}, on input $S\in {[n] \choose m}$, $\enc$ computes $M \leftarrow \sketch(\mathbf{1}_S)$ as part of its output. Moreover, $\enc$ also outputs a subset $B\subseteq S$ computed as follows. Initially $B=S$ and $S_0=S$. $\enc$ proceeds in $R$ rounds.  In round $r\in[R]$, $\enc$ computes $s_r\leftarrow \query(M, \mathbf{1}_{S\backslash S_{r-1}})$.  Let $b$ denote a binary string of length $R$, where $b_r$ records whether $\query$ succeeds in round $r$.  $\enc$ also outputs $b$.  If $s_r\in S_{r-1}$, i.e. $\query(M, \mathbf{1}_{S\backslash S_{r-1}})$ succeeds, $\enc$ sets $b_r=1$ and removes $s_r$ from $B$ (since the decoder can recover $s_r$ from the $\ur^\subset$-protocol, $\enc$ does not need to include it in $B$); otherwise $\enc$ sets $b_r=0$.  At the end of round $r$, $\enc$ picks a uniformly random set $S_r$ in $\binom{S_{r-1}\backslash \{s_r\}}{n_r}$.  In particular, $\enc$ uses its shared randomness with $\dec$ to generate $S_r$ in such a way that $\enc, \dec$ agree on the sets $S_r$ ($\dec$ will actually iteratively construct $C_r = S\backslash S_r$). We present $\enc$ in Algorithm~\ref{algo:enc}.

The decoding process is symmetric.  Let $C_0=\emptyset$ and $A=\emptyset$.  $\dec$ proceeds in $R$ rounds.  On round $r\in[R]$, $\dec$ obtains $s_r\in S\backslash C_{r-1}$ by invoking $\query(M, \mathbf{1}_{C_{r-1}})$.  By construction of $C_{r-1}$ (to be described later), it is guaranteed that $S_{r-1}=S\backslash C_{r-1}$.  Therefore, $\dec$ recovers exactly the same $s_r$ as $\enc$.  $\dec$ initially assigns $C_r\leftarrow C_{r-1}$.  If $b_r=1$, $\dec$ adds $s_r$ to both $A$ and $C_r$.  At the end of round $r$, $\dec$ inserts many random items from $B$ into $C_r$ so that $C_r=S\backslash S_r$.  $\dec$ can achieve this because of the shared random permutation $\pi$ when constructing $S_r$.  In the end, $\dec$ outputs $B\cup A$.  We present $\dec$ in Algorithm~\ref{algo:dec}.

\begin{algorithm}[H] 
  \caption{Variables Shared by encoder $\enc$ and decoder $\dec$.} \label{algo:para}
  \begin{algorithmic}[1] 
    \State $m\leftarrow \lfloor \sqrt{n \log\frac{1}{\delta}} \rfloor$ 
    \State $K\leftarrow \lfloor \frac{1}{16}\log \frac{1}{\delta} \rfloor$
    \State $R\leftarrow \lfloor K\log(m/4K) \rfloor$
    \For {$r = 0, \ldots, R$}
      \State $n_r\leftarrow \lfloor m \cdot 2^{-\frac{r}{K}} \rfloor$ \Comment{$|S_r|=n_r$, and we have $n_r-n_{r+1}\ge 2$}
    \EndFor
    \State Let $\pi$ be a random permutation on $[n]$ \Comment{Used to generate $S_r$ and $C_r$}
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Encoder $\enc$.} \label{algo:enc}
  \begin{algorithmic}[1]
    \Procedure{$\enc$}{$S$}
    \State $M \leftarrow \sketch(\mathbf{1}_S)$
    \State $A\leftarrow \emptyset$
    \State $S_0 \leftarrow S$
    \For {$r=1,\ldots,R$}
      \State $s_r\leftarrow \query(M, \mathbf{1}_{S\backslash S_{r-1}})$
      \State $S_r\leftarrow S_{r-1}$
      \If {$s_r\in S_{r-1}$} \Comment{i.e. if $s_r$ is a valid sample}
        \State $b_r\leftarrow 1$ \Comment{$b$ is a binary string of length $R$, indicating if $\query$ succeeds on round $r$}
        \State $A\leftarrow A \cup \{s_r\}$
        \State $S_r\leftarrow S_r \backslash \{s_r\}$
      \Else 
        \State $b_r\leftarrow 0$
      \EndIf
      \State Remove $|S_r|-n_r$ elements from $S_r$ with smallest $\pi_a$'s among $a\in S_r$ \Comment{So that $|S_r|=n_r$}
    \EndFor
    \State \Return ($M$, $S\backslash A$, $b$) 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Decoder $\dec$.} \label{algo:dec}
  \begin{algorithmic}[1]
    \Procedure{$\dec$}{$M$, $B$, $b$}
    \State $A\leftarrow \emptyset$
    \State $C_0 \leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
      \State $C_r\leftarrow C_{r-1}$
      \If{$b_r=1$}
        \State $s_r\leftarrow \query(M, \mathbf{1}_{C_{r-1}})$ \Comment{Invariant: $C_r=S \backslash S_r$ ($S_r$ is defined in $\enc$)}
        \State $A\leftarrow A \cup \{s_r\}$
        \State $C_r\leftarrow C_r \cup \{s_r\}$
      \EndIf
       \State Insert $m-n_r-|C_r|$ items into $C_r$ with smallest $\pi_a$'s among $a\in B\backslash C_r$
    \EndFor
    \State \Return $B\cup A$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Analysis}

We have two random objects in our encoding/decoding scheme: (1) the random source used by $\mathcal{P}$, denoted by $X$, and (2) the random permutation $\pi$. These are independent.

First, we can prove that $\dec(\enc(S))=S$.  That is, for any fixing of the randomness in $X$ and $\pi$, $\dec$ will always decode $S$ successfully.  It is because $\enc$ and $\dec$ share $X$ and $\pi$, so that $\dec$ essentially simulates $\enc$.  We formally prove this by induction in Lemma~\ref{lemma:zero-fail-prob}.

Now our goal is to prove that by using the $\ur^\subset$-protocol, the number of bits that $\enc$ saves in expectation over the naive $\lceil\log(^n_m)\rceil$-bit encoding is $\Omega(\log \frac{1}{\delta}\log^2 \frac{n}{\log (1/\delta)} )$ bits.  Intuitively, it is equivalent to prove the number of elements that $\enc$ saves is $\Omega(\log \frac{1}{\delta}\log \frac{n}{\log (1/\delta)} )$.
We formalize this in Lemma~\ref{lemma:bits-saving}. 
Note that $\enc$ also needs to output $b$ (i.e., whether the $\query$ succeeds on $R$ rounds), which takes $R$ bits. 
By our setting of parameters, we can afford the loss of $R$ bits.  Thus it is sufficient to prove $\E|B|=|S|-\Omega(\log \frac{1}{\delta}\log \frac{n}{\log (1/\delta)})$. 

We have $|S|-|B|=\sum_{r=1}^{R}b_r$. 
In Lemma~\ref{lem:information}, we prove the probability that $\query$ fails on round $r$ is upper bounded by $\frac{I(X;S_{r-1})+1}{\log \frac{1}{\delta}}$, where $I(X;S_{r-1})$ is the mutual information between $X$ and $S_{r-1}$. 
Furthermore, we will show in Lemma~\ref{lemma:mutual-entropy-bound} that $I(X;S_{r-1})$ is upper bounded by $O(K)$.
By our setting of parameters, we have $\E b_r=\Omega(1)$ and thus $\E(|S|-|B|)=\Omega(R)=\Omega(\log \frac{1}{\delta}\log \frac{n}{\log (1/\delta)})$.
 
\begin{lemma}\label{lemma:zero-fail-prob}
  $\dec(\enc(S))=S$.
\end{lemma}
\begin{proof}
  We claim that for $r=0,\ldots, R$, $\{S_r, C_r\}$ is a partition of $S$ ($S_r$ is defined in Algorithm~\ref{algo:enc}, and $C_r$ in Algorithm~\ref{algo:dec}). We prove the claim by induction on $r$. Our base case is $r=0$, for which the claim holds since $S_0 = S$, $C_0 = \emptyset$.
  
  Assume the claim holds for $r$ ($0\le r < R$), and we consider round $r+1$.  On round $r$, because $S\backslash S_{r-1}=C_{r-1}$, the index $s_r$ obtained by both \enc and \dec are the same.  Initially $S_r=S_{r-1}$ and $C_r=C_{r-1}$, and so $\{S_r,C_r\}$ is a partition of $S$.  If $s_r$ is a valid sample (i.e. $s_r\in S_{r-1}$), then $b_r=1$, and \enc removes $s_r$ from $S_r$ and in the meanwhile \dec inserts $s_r$ into $C_r$, so that $\{S_r, C_r\}$ remains a partition of $S$. Next, \enc repeats removing the $a$ from $S_r$ with the smallest $\pi_a$ value until $|S_r|=n_r$. Symmetrically, \dec repeats inserting the $a$ into $C_r$ with the smallest $\pi_a$ value among $a\in B\backslash C_r$, until $|C_r|=|S|-n_r$. In the end we have $|S_r|+|C_r|=|S|$, so \enc and \dec execute repetition the same number of times.  Moreover, we can prove that during the same iteration of this repreated insertion, the element removed from $S_r$ is exactly the same element inserted to $C_r$.  This is because in the beginning of a repetition $\{S_r, C_r\}$ is a partition of $S$.  We have $B\backslash C_r\subseteq S\backslash C_r=S_r$. Let $a^*$ denote $a\in S_r$ that minimizes $\pi_a$.  Then $a^*\in B\backslash C_r\subseteq S_r$ (since $a^*$ will be removed from $S_r$, it has no chance to be included in $S$ in \enc, so that $B$ contains $a^*$), and $\pi_{a^*}$ is also the smallest among $\{\pi_a : a\in B\backslash C_r\}$.  Thus both $\enc$ and $\dec$ will take $a^{*}$ (for \enc, to remove from $S_r$, and for \dec, to insert into $C_r$).  Therefore, $\{S_r, C_r\}$ remains a partition of $S$.
  
  Given the fact that $\{S_r, C_r\}$ is a partition of $S$, the $s_r$ are the same in \enc and \dec.  Furthermore, $A=\{s_r : b_r=1,r=1,\ldots, R\}$ are the same in \enc and \dec.  We know $A\subseteq S$.  Since \enc outputs $S\backslash A$, and \dec outputs $(S\backslash A)\cup A$, we have $\dec(\enc(S))=S$.
\end{proof}

\begin{lemma} \label{lemma:bits-saving}
Let $W\in \mathbb{N}$ be a random variable with $W\le m$ and $\E W\le m-d$. Then $\E(\log {n \choose m}-\log {n \choose W})\ge d \log (\frac{n}{m}-1)$.
\end{lemma}

\begin{proof}
  \begin{align*}
  \log {n \choose m}-\log {n \choose W}
  &= \log \frac{n!/(m!(n-m)!)}{n!/(W!(n-W)!)} \\
  &= \sum_{i=1}^{m-W}\log \frac{n-W-i+1}{m-i+1} \\
  &\ge (m-W)\cdot \log \frac{n-W}{m} \\
  &\ge (m-W)\cdot \log \frac{n-m}{m}
  \end{align*}
  
  Taking expectation on both sides, we have $\E(\log {n \choose m}-\log {n \choose W})\ge d \log (\frac{n}{m}-1)$. 
\end{proof}

\noindent \textbf{Lemma~\ref{lem:information} (restated).}
  Consider $f$: $\{0,1\}^b\times \{0,1\}^q\rightarrow \{0,1\}$ and $X\in\{0,1\}^b$ uniformly random. If $\forall y\in \{0,1\}^q,\ \Pr(f(X,y)=1)\le \delta$ where $0<\delta<1$, then for any r.v.\ $Y$ supported on $\{0,1\}^q$,
$$
  \Pr(f(X,Y)=1)\le \frac{I(X;Y)+1}{\log \frac{1}{\delta}} ,
$$
  where $I(X;Y)$ is the mutual information between $X$ and $Y$.
\begin{proof}
  It is equivalent to prove 
$$I(X;Y)\ge \E(f(X,Y))\cdot \log\frac{1}{\delta}-1 .$$
By definition of mutual entropy $I(X;Y)=H(X)-H(X|Y)$, where $H(X)=b$ and 
$$H(X|Y)\le 1+(1-\E(f(X,Y)))\cdot b+\E(f(X,Y))\cdot (b-\log\frac{1}{\delta})=b+1-\E(f(X,Y))\cdot \log\frac{1}{\delta} .$$
  The upper bound for $H(X|Y)$ is obtained by considering the following one-way communication problem: Alice knows both $X$ and $Y$ while Bob only knows $Y$, and Alice must send a single message to Bob so that Bob can recover $X$. The expected message length in an optimal protocol is exactly $H(X|Y)$.  Thus, any protocol gives an upper bound for $H(X|Y)$, and we simply take the following protocol: first Alice sends Bob $f(X,Y)$ (taking $1$ bit). Then if $f(X,Y)=0$, Alice sends $X$ directly (taking $b$ bits). Otherwise, when $f(X,Y)=1$, Alice sends the index of $X$ in $\{x|f(x,Y)=1\}$ (taking $\log (\delta 2^b)=b-\log\frac{1}{\delta}$ bits).  
\end{proof}

\begin{corollary}\label{corollary:sampler-failure}
  Let $X$ denote the random source used by the $\ur^\subset$-protocol with failure probability at most $\delta$. If $S$ is a fixed set and $T\subset S$, $\Pr(\query(\sketch(\mathbf{1}_S), \mathbf{1}_T)\not\in S\backslash T)\le \frac{I(X;T)+1}{\log\frac{1}{\delta}}$.
\end{corollary}

\begin{lemma}\label{lemma:mutual-entropy-bound}
  $I(X;S_r)\le 6K$, for $r=1,\ldots, R$.
\end{lemma}

\begin{proof}
  $I(X;S_r)=H(S_r)-H(S_r|X)$. Since $|S_r|=n_r$ with $S_r\subseteq S$, $H(S_r)\le \log {m \choose n_r}$. Now we want to lower bound $H(S_r|X)$. By definition of conditional entropy, $H(S_r|X)=\sum_x{p_x\cdot H(S_r|X=x)}$. We fix an arbitrary $x$. If we can prove that for any $T\subseteq S$ where $|T|=n_r$, $\Pr(S_r=T|X=x)\le p$, then by definition of entropy we have $H(S_r|X=x)\ge\log\frac{1}{p}$. In fact, for any fixed $T$, we have
  
  \begin{align}
    \Pr(S_r=T|X=x)\le \prod_{i=1}^{r}{\frac{{n_{i-1}-n_r-1 \choose n_{i-1}-n_i-1}}{{n_{i-1}-1 \choose n_{i-1}-n_i-1}}},
  \end{align}
   because on round $i$ ($1\le i \le r$), $\enc$ removes $n_{i-1}-n_i$ elements from $S_{i-1}$ to obtain $S_i$. Conditioned on the event that $S_{i-1}\supseteq T$, the probability that $S_i\supseteq T$ is at most ${{n_{i-1}-n_r-1 \choose n_{i-1}-n_i-1}}/{{n_{i-1}-1 \choose n_{i-1}-n_i-1}}$, where the equation achieves when $s_i\in S_{i-1}\backslash T$, and $\enc$ takes a uniformly random subset of $S_{i-1}\backslash \{s_i\}$ of size $n_{i-1}-n_i-1$, so that the subset does not intersect with $T$.
  
  For notational simplicity, let $n^{\underline{k}}$ denote $n\cdot (n-1)\ldots (n-k+1)$. We have 
  \begin{align}
    \prod_{i=1}^{r}{\frac{{n_{i-1}-n_r-1 \choose n_{i-1}-n_i-1}}{{n_{i-1}-1 \choose n_{i-1}-n_i-1}}}
    =\prod_{i=1}^{r}\frac{(n_{i-1}-n_r-1)!n_i!}{(n_{i-1}-1)!(n_i-n_r)!}
    =\prod_{i=1}^{r}\frac{n_i^{\underline{n_r}}}{(n_{i-1}-1)^{\underline{n_r}}}
    =\prod_{i=1}^{r} \left( \frac{n_i^{\underline{n_r}}}{n_{i-1}^{\underline{n_r}}}\cdot \frac{n_{i-1}}{n_{i-1}-n_r} \right).
  \end{align}
  
  By telescoping,
  \begin{align}
    \prod_{i=1}^{r} \frac{n_i^{\underline{n_r}}}{n_{i-1}^{\underline{n_r}}}
    =\frac{n_r^{\underline{n_r}}}{n_0^{\underline{n_r}}}
    =\frac{n_r!(n_0-n_r)!}{n_0!}=\frac{1}{{n_0 \choose n_r}}
    =\frac{1}{{m \choose n_r}}.
  \end{align}
  
  Moreover, 
  \begin{align}
    \prod_{i=1}^{r} \frac{n_{i-1}}{n_{i-1}-n_r}
    \le\prod_{i=1}^{r} \frac{1}{1-\frac{m\cdot 2^{-r/K}}{m\cdot 2^{-(i-1)/K}-1}}
    \le\prod_{i=1}^{r} \frac{1}{1-\frac{m\cdot 2^{-r/K}+1}{m\cdot 2^{-(i-1)/K}}}
    =\prod_{j=1}^{r} \frac{1}{1-2^{-j/K}-\frac{2^{\frac{r-j}{K}}}m}.
  \end{align}
  
  By our setting of parameters 
$$\frac{2^{\frac rK}}m \le \frac{2^{\frac RK}}m \le \frac{1}{4K} .$$
  Therefore, 
$$\frac{1}{1-2^{-\frac jK}-\frac{2^{\frac{r-j}{K}}}m}\le \frac{1}{1-(1+\frac{1}{4K})2^{-\frac jK}}$$ 
for $j\in[r]$. First, we can prove 
$$\frac{1}{1-(1+\frac{1}{4K})2^{-\frac 1K}}\le 2K \le 2^K .$$
  Also by Taylor expansion we can obtain $1+\frac{1}{4K}\le 2^{-1/K}$, and thus $\frac{1}{1-(1+\frac{1}{4K})2^{-j/K}}\le \frac{1}{1-2^{(1-j)/K}}$, for $j=2,\ldots, r$. 
  By Lemma~\ref{lemma:Pochhammer}, we have $\prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}\le 2^{5K}$. 
  
  Let $p={2^{6K}}/{{m\choose n_r}}$, we have $\Pr(S_r=T|X=x)\le p$ and thus $H(S_r|X=x)\ge \log\frac{1}{p}=\log{{m\choose n_r}}-6K$. Therefore, $H(S_r|X)\ge \log{{m\choose n_r}}-6K$ and so $I(X;S_r)=H(S_r)-H(S_r|X)\le 6K$.  
\end{proof}

%By \url{http://mathworld.wolfram.com/q-PochhammerSymbol.html} We have $\prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}\le 2^{5K}$. I think we can improve the constant 5 to 4 if we bound it more carefully. 
\begin{lemma}\label{lemma:Pochhammer}
  Let $K\in \mathbb{N}$ and $K\ge 1$. We have $\prod_{j=1}^{\infty} \frac{1}{1-2^{-j/K}}\le 2^{5K}$.
\end{lemma}

\begin{proof}
  First, we bound the product of first $2K$ terms. Note that $\frac{1}{1-2^{-x}}\le \frac{8}{3x}$ for $0<x\le 2$. Therefore, 
  \begin{align}
    \prod_{j=1}^{2K}\frac{1}{1-2^{-j/K}}
    \le (8/3)^{2K}\cdot \frac{K^{2K}}{(2K)!}
    \le (8/3)^{2K}\cdot \frac{K^{2K}}{(2K/e)^{2K}}
    = (4e/3)^{2K}
    < 2^{4K}. 
  \end{align}
  
  Then, we bound the product of the rest terms
  \begin{align}
    \prod_{j=2K+1}^{\infty}\frac{1}{1-2^{-j/K}} 
    \le \prod_{j=2K+1}^{\infty}\frac{1}{1-2^{-\lfloor j/K \rfloor}} 
    \le \prod_{i=2}^{\infty}\left( \frac{1}{1-2^{-i}}\right)^K 
    \le \left( \frac{1}{1-\sum_{i=2}^{\infty}2^{-i}}\right)^K
    = 2^K.
  \end{align}
  
  Multiplying two parts proves the lemma.
\end{proof}

\begin{theorem}
  $\randcom^{\rightarrow,pub}_\delta(\ur^\subset) = \Omega(\log \frac{1}{\delta}\log^2 \frac{n}{\log (1/\delta)} )$, given that $64 \le \log \frac{1}{\delta} \le \frac{n}{64}$.
\end{theorem}

\begin{proof}
  By Lemma~\ref{lemma:zero-fail-prob}, the success probability of protocol $(\enc,\dec)$ is $1$. 
  By Lemma~\ref{lemma:lb-meta}, we have $\s\ge \log (^n_m) - \s' -1$, where $\s'=\log n + R+ \E(\log (^n_{|B|}))$. 
  The size of $B$ is $|B|=|S|-\sum_{r=1}^{R}{b_r}$.
  By Corollary~\ref{corollary:sampler-failure}, conditioned on $S$, $\Pr(b_r=0)\le \frac{I(X;S_{r-1})+1}{\log\frac{1}{\delta}}$. 
  By Lemma~\ref{lemma:mutual-entropy-bound}, $I(X;S_{r-1})\le 6K$ (Note that when $r=1$, $I(X;S_0)=0\le 6K$). 
  Therefore, $\E(b_r)\ge 1-\frac{6K+1}{\log\frac{1}{\delta}}$.
  By the setting of parameters (see Algorithm~\ref{algo:para}) we have $\E(b_r)\ge \frac{39}{64}$. Therefore, $\E(|B|)\le |S|-\frac{39}{64}R$. 
  By Lemma~\ref{lemma:bits-saving}, $\log (^n_m)-\E(\log (^n_{|B|}))\ge \frac{39}{64}R\cdot \log (\frac{n}{m}-1) \ge \frac{1}{2}R\log (\frac{n}{\log(1/\delta)})$. 
  Furthermore, $\frac{1}{6}R\log \frac{n}{\log (1/\delta)} \ge R$.
  Thus we obtain $\s \ge \frac{R}{3}\log \frac{n}{\log(1/\delta)} -(\log n + 1)  =\Omega(\log \frac{1}{\delta}\log^2 \frac{n}{\log (1/\delta)} )$.
\end{proof}