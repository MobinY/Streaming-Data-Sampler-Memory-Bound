\section{Lower Bound for Obtaining Multiple Samples}\label{sec:k-samples-lb}

In this section, we prove a space lower bound of $\Omega(k\log^2
\frac{n}{k})$ bits for samplers that can give $k$ samples (where $1\le k \le \frac{n}{2^{10}}$) with failure probability at most $\delta = \frac{1}{2}$.
We consider the following encoding scheme. 
Alice wants to send Bob set $A$, where $|A|=m$. 
She will insert all the elements in $A$ into sampler $\samp$, and send $\samp$ to Bob. 
In addition, Alice will send Bob $B\subseteq A$ constructed as follows.
Initially $B\leftarrow A$, and Alice proceeds in $R=\Theta(\log (n/k))$ rounds. 
Let $A_0=A\supseteq A_1\supseteq \ldots \supseteq A_R$ where $A_r$ is generated by sub-sampling each element in $A_{r-1}$ with probability $\frac{1}{2}$. 
On round $r$ ($r=1,\ldots, R$), Alice tries to get $k$ samples from $A_{r-1}$, denoted by $S_k$, and removes $S_k\cap (A_{r-1}\backslash A_{r})$ from $B$. 
Note that Bob is able to recover the elements in $S_k\cap (A_{r-1}\backslash A_{r})$ whose expected size is $\frac{k}{2}$, and for each round the sampler's failure probability is at most $\delta$. 
Thus we have $\E(|A|-|B|)\ge \frac{k}{2}\cdot (1-\delta) \cdot R=\Omega(k\log\frac{n}{k})$. By setting of parameters each item contains $\Theta(\log \frac{n}{k})$ bits of information, thus yielding a lower bound of $\Omega(k\log^2\frac{n}{k})$ bits.

\subsection{Protocol}
\begin{algorithm}[H] 
  \caption{Variables Shared by Alice's $\enc_4$ and Bob's $\dec_4$.} \label{algo:para4}
  \begin{algorithmic}[1] 
    \State $m\leftarrow \lfloor \sqrt{nk} \rfloor$
    \State $R\leftarrow \lfloor \frac{1}{2}\log (n/k) - 2 \rfloor$ \Comment{Note that $R\ge 3$ because $k\le \frac{n}{2^{10}}$}
    \State $T_0\leftarrow [n]$
    \For {$r = 1, \ldots, R$}
      \State $T_r\leftarrow \emptyset$
      \State For each $a\in T_{r-1}$, $T_r\leftarrow T_r\cup \{a\}$ with probability $\frac{1}{2}$ \Comment{We have $A_r=A\cap T_r$}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Alice's Encoder.} \label{algo:enc4}
  \begin{algorithmic}[1]
    \Procedure{$\enc_4$}{$A$}
    \State $\sk \leftarrow \sketch(\chi_A)$
    \State $S\leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
    \State $S_r\leftarrow \query_k(\sk, \chi_{A\backslash (A\cap T_{r-1})})$
    \If {$S_r\subseteq A\cap T_{r-1}$} \Comment{i.e. if $S_r$ are valid}
      \State $b_r\leftarrow 1$ \Comment{$b$ is a binary string of length $R$, indicating if sampler succeeds on round $r$}
      \State $S\leftarrow S \cup (S_r\cap (T_{r-1}\backslash T_r))$
    \Else 
      \State $b_r\leftarrow 0$
    \EndIf
    \EndFor
      \State \Return ($\sk$, $A\backslash S$, $b$) 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Bob's Decoder.} \label{algo:dec4}
  \begin{algorithmic}[1]
    \Procedure{$\dec_4$}{$\sk$, $B$, $b$}
    \State $S\leftarrow \emptyset$
    \State $C_0 \leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
      \State $C_r\leftarrow C_{r-1}$
      \If {$b_r=1$}
        \State $S_r\leftarrow \query_k(\sk, \chi_{C_{r-1}})$ \Comment{Invariant: $C_r=A\backslash (A\cap T_r)$}
        \State $S\leftarrow S \cup (S_r\cap (T_{r-1}\backslash T_r))$
        \State $C_r\leftarrow C_r \cup (S_r\cap (T_{r-1}\backslash T_r))$
      \EndIf
      \State $C_r\leftarrow C_r \cup (B\cap (T_{r-1}\backslash T_r))$
    \EndFor
    \State \Return $B\cup S$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Analysis}

\begin{theorem}
  $\s = \Omega(k\log^2 \frac{n}{k} )$, given that $1 \le k \le \frac{n}{2^{10}}$ and $\delta \le \frac{1}{2}$.
\end{theorem}

\begin{proof}
  Let $A_r=A\cap T_r$. 
  Let $\success$ denote the event that $|A\cap T_R|=|A_R|\ge k$. 
  Note that $\E(|A_R|)=\frac{1}{2^R}m=4k$. By Chernoff bound, $\Pr(\success)\ge \frac{1}{2}$. 
  In the following, we argue conditioned on $\success$. Namely, on each round $r$, there is at least $k$ items in $A_r$.  
  
  Similar to Lemma~\ref{lemma:zero-fail-prob}, we can prove the protocol $(\enc_4,\dec_4)$ always succeeds. 
  By Lemma~\ref{lemma:lb-meta}, we have $\s\ge \log (^n_m) - \s' -2$, where $\s'=\log n + R+ \E(\log (^n_{|B|}))$. 
  The size of $B$ is $|B|=|A|-\sum_{r=1}^{R}{(b_r \cdot |S_r \cap (A_{r-1}\backslash A_r)|)}$.
  Conditioned on $A$, the randomness of updates to $\samp_r$ comes from $A_{r-1}$ (for $r=1, \ldots, R$), which is independent from the random source used by the sampler.
  Therefore, $\E(b_r)\ge 1-\delta\ge \frac{1}{2}$, and $b_r$ is independent from $|S_r \cap (A_{r-1}\backslash A_r)|$. 
  We have $\E(|S_r \cap (A_{r-1}\backslash A_r)|)=\frac{k}{2}$, and thus $\E(|A|-|B|)\ge \frac{kR}{4}$. 
  By Lemma~\ref{lemma:bits-saving}, $\log (^n_m)-\E(\log (^n_{|B|}))\ge \frac{kR}{4}\cdot \log (\frac{n}{m}-1) \ge \frac{kR}{9}\log (\frac{n}{k})$.
  Moreover, $\frac{kR}{10}\log \frac{n}{k}\ge R$.  
  Combining together we get $\s = \Omega(kR\log\frac{n}{k}) = \Omega(k\log^2 \frac{n}{k} )$.
\end{proof}
