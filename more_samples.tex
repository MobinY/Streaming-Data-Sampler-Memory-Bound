\section{Lower Bound for Obtaining Multiple Samples}\label{sec:k-samples-lb}

In this section, we prove a space lower bound of $\Omega(k\log^2
\frac{n}{k})$ bits for samplers that can give $k$ samples with failure probability at most $\delta = \frac{1}{2}$. 
Alice wants to send Bob set $A$, where $|A|=m$. She will insert all the elements in $A$ to sampler $\samp$, and send $\samp$ to Bob. 
In addition, Alice will send Bob $B\subseteq A$ constructed as follows.
Initially $B\leftarrow A$, and Alice proceeds in $R=\Theta(\log (n/k))$ rounds. 
Let $A_0=A\supseteq A_1\supseteq \ldots$ where each set is generated by sub-sampling the elements in previous one with probability $\frac{1}{2}$. 
On round $r$ ($r=1,\ldots, R$), Alice obtains $k$ samples from $A_{r-1}$, denoted by $S_k$, and removes $S_k\cap (A_{r-1}\backslash A_{r})$ from $B$. 
Note that Bob is able to recover the elements in $S_k\cap (A_{r-1}\backslash A_{r})$ whose expected size is $\frac{k}{2}$, and for each round the sampler's failure probability is at most $\delta$. 
Thus we have $\E(|A|-|B|)\ge \frac{k}{2}\cdot (1-\delta) \cdot R=\Omega(k\log\frac{n}{k})$. By setting of parameters each items contain $\Theta(\log \frac{n}{k})$ bits of information, thus it yields a lower bound of $\Omega(k\log^2\frac{n}{k})$ bits.

\subsection{Protocol}
\begin{algorithm}[H] 
  \caption{Variables Shared by Alice's $\enc_4$ and Bob's $\dec_4$.} \label{algo:para4}
  \begin{algorithmic}[1] 
    \State $m\leftarrow \sqrt{nk}$
    \State $R\leftarrow \frac{1}{2}\log (n/k) - 2$
    \State $T_0\leftarrow [n]$
    \For {$r = 1, \ldots, R$}
      \State $T_r\leftarrow \emptyset$
      \State For each $a\in T_{r-1}$, $T_r\leftarrow T_r\cup \{a\}$ with probability $\frac{1}{2}$
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Alice's Encoder.} \label{algo:enc4}
  \begin{algorithmic}[1]
    \Procedure{$\enc_4$}{$A$}
    \State Initialize an $\ell_0$-sampler $\samp$
    \State Insert items in $A$ into \samp
    \State $S\leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
    \State Let $\samp_r$ be a copy of $\samp$
    \State Remove items in $A\backslash (A\cap T_{r-1})$ from $\samp_r$ \Comment Now $\samp_r$ contains the elements in $A\cap T_{r-1}$
    \State Let $S_r$ be $k$ samples obtained from $\samp_r$
    \If {$S_r\subseteq A\cap T_{r-1}$} \Comment{i.e. if $S_r$ are valid samples}
      \State $b_r\leftarrow 1$ \Comment{$b$ is a binary string of length $R$, indicating if sampler succeeds on round $r$}
      \State $S\leftarrow S \cup (S_r\cap (T_{r-1}\backslash T_r))$
    \Else 
      \State $b_r\leftarrow 0$
    \EndIf
    \EndFor
      \State \Return ($A\backslash S$, $b$, \samp) 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
  \caption{Bob's Decoder.} \label{algo:dec4}
  \begin{algorithmic}[1]
    \Procedure{$\dec_4$}{$B$, $b$, \samp}
    \State $S\leftarrow \emptyset$
    \State $C_0 \leftarrow \emptyset$
    \For {$r=1,\ldots,R$}
      \State $C_r\leftarrow C_{r-1}$
      \If {$b_r=1$}
        \State Let $\samp_r$ be a copy of $\samp$
        \State Remove items in $C_{r-1}$ from $\samp_r$ \Comment{Invariant: $C_r=A\backslash (A\cap T_r)$}
        \State Let $S_r$ be $k$ samples from $\samp_r$
        \State $S\leftarrow S \cup (S_r\cap (T_{r-1}\backslash T_r))$
        \State $C_r\leftarrow C_r \cup (S_r\cap (T_{r-1}\backslash T_r))$
      \EndIf
      \State $C_r\leftarrow C_r \cup (B\cap (T_{r-1}\backslash T_r))$
    \EndFor
    \State \Return $B\cup S$ 
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Analysis}

\begin{theorem}
  $\s = \Omega(k\log^2 \frac{n}{k} )$, given that $2^{10} \le k \le \frac{n}{2^{10}}$.
\end{theorem}

\begin{proof}
  Let $A_r=A\cap T_r$. 
  Let $\success$ denote the event that $|A\cap T_R|=|A_R|\ge k$. 
  Note that $\E(|A_R|)=\frac{1}{2^R}m=4k$. By Chernoff bound, with good probability $\success$ happens. 
  In the following, we argue conditioned on $\success$ happens. 
  Similar to Lemma~\ref{lemma:zero-fail-prob}, the protocol $(\enc_4,\dec_4)$ always succeeds. 
  By Lemma~\ref{lemma:lb-meta}, we have $\s\ge \log (^n_m) - \s' -2$, where $\s'=\log n + R+ \E(\log (^n_{|B|}))$. 
  The size of $B$ is $|B|=|A|-\sum_{r=1}^{R}{(b_r \cdot |S_r \cap (A_{r-1}\backslash A_r)|)}$.
  Conditioned on $A$, the randomness of updates to $\samp_r$ comes from $A_{r-1}$ (for $r=1, \ldots, R$), which is independent from the random source used by the sampler.
  Therefore, $\E(b_r)\ge 1-\delta\ge \frac{1}{2}$, and $b_r$ is independent from $|S_r \cap (A_{r-1}\backslash A_r)|$. 
  We have $\E(|S_r \cap (A_{r-1}\backslash A_r)|)=\frac{k}{2}$, and thus $\E(|A|-|B|)\ge \frac{kR}{4}$. 
  By Lemma~\ref{lemma:bits-saving}, $\log (^n_m)-\E(\log (^n_{|B|}))\ge \frac{kR}{4}\cdot \log (\frac{n}{m}-1) \ge \frac{kR}{10}\log (\frac{n}{k})$. 
  Combining together we get $\s = \Omega(k\log^2 \frac{n}{k} )$.
\end{proof}
