\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{algorithm,algpseudocode,float}
\usepackage{xspace}
\usepackage[usenames]{color}
%\usepackage{hyperref}

\newcommand{\TODO}[1]{\textcolor{red}{\textbf{todo:} \textit{#1}}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\newcommand{\samp}{\textsf{SAMP}\xspace}
\newcommand{\success}{\textsf{SUCC}\xspace}
\newcommand{\enc}{\textsf{ENC}\xspace}
\newcommand{\dec}{\textsf{DEC}\xspace}
\newcommand{\s}{\textsf{s}\xspace}
\newcommand{\R}{\mathbb{R}}
\newcommand{\sk}{\mathsf{sk}}
\newcommand{\eps}{\varepsilon}

\title{An optimal space lower bound for samplers}
\author{Jelani Nelson\thanks{Harvard University. \texttt{minilek@seas.harvard.edu}. Supported by NSF grant IIS-1447471 and
   CAREER award CCF-1350670, ONR Young Investigator award N00014-15-1-2388, and a Google Faculty Research Award.}
  \and Jakub Pachocki\thanks{OpenAI. \texttt{jakub.pachocki@gmail.com}. Work done while affiliated with Harvard University, with the support of ONR grant N00014-15-1-2388.}
  \and Zhengyu Wang\thanks{Harvard University. \texttt{zhengyuwang@g.harvard.edu}. Supported by NSF grant CCF-1350670.}}

\begin{document}

\maketitle

\begin{abstract}
In the $\ell_0$-sampler sketching problem, the goal is to compress a high-dimensional vector $x\in\R^n$ into a low-dimensional {\em sketch} $\sk(x)\in\{0,1\}^S$ so that
\begin{enumerate}
\item $S$, the memory footprint of the sketch, is as small as possible,
\item a third party given only $\sk(x), \sk(y)$, but not $x, y\in\R^n$, is able to form $\sk(x+y)$ for any $x,y\in\R^n$,
\item given access to only $\sk(x)$, a third party can recover an index $i\in[n]$ such that with probability $1-\delta$, $i$ is a uniformly random element in $\mathop{support}(x) = \{i : x_i\neq 0\}$.
\end{enumerate}
We prove that any such sketching mechanism requires $S = \Omega(\log^2 n\cdot \log(1/\delta))$ bits for any $2^{-n^{.99}} < \delta < .1$, which is nearly the full range of $\delta$ of interest and is optimal in this range \cite{JowhariST11}. Our lower bound holds even if the mechanism is allowed to output any element of the support, and not necessarily a uniformly random one, and thus the lower bound also holds against $\ell_p$-sampling for any $p$, and is optimal for $0<p<2$ for constant error parameter $\eps$ given the upper bound of \cite{JowhariST11}. 

We prove our lower bound by showing that $\ell_0$-samplers which are {\em too} memory-efficient can be used to compress subsets of $[n]$ of certain sizes below the information theoretic minimum. Our encoding scheme makes adaptive queries to the sampler throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's queries to the sampler, and is loosely motivated by ideas in differential privacy. Our correctness analysis involves understanding the ability of the sampler to correctly answer adaptive queries which have non-zero, but bounded, mutual information with the randomness of the mechanism, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens.
\end{abstract}

\input{intro.tex}
\input{warmup.tex}
\input{main.tex}
\input{more_samples.tex}

\bibliographystyle{alpha}
\bibliography{shortbib}

\end{document}
























